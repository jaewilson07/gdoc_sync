# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/llm/llm_qna.ipynb.

# %% auto 0
__all__ = ["get_llm_response"]

# %% ../../nbs/llm/llm_qna.ipynb 2
from langchain_openai import ChatOpenAI

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory

# %% ../../nbs/llm/llm_qna.ipynb 4
llm = ChatOpenAI(
    openai_api_base="http://172.20.2.239:1234/v1/",
    openai_api_key="not-needed",
    model="local_model",
    callbacks=[StreamingStdOutCallbackHandler()],
)


# %% ../../nbs/llm/llm_qna.ipynb 5
def get_llm_response(
    user_content,
    system_prompt="You are a QandA Bot",
    history=None,
) -> AIMessage:
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_content),
    ]

    return llm.stream(
        messages,
        temperature=0.7,
        stream=True,
    )
