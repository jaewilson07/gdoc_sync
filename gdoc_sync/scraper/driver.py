# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/scraper/driver.ipynb.

# %% auto 0
__all__ = ['DriverGenerator_NotInPath', 'DriverGenerator', 'get_pagesource']

# %% ../../nbs/scraper/driver.ipynb 2
import re

import subprocess

from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

from bs4 import BeautifulSoup

# %% ../../nbs/scraper/driver.ipynb 8
class DriverGenerator_NotInPath(Exception):
    def __init__(self):
        super().__init__(
            "google-chrome not found in PATH and therefore cannot be accessed by a driver.  install it or add to PATH"
        )


class DriverGenerator:
    """class for generating drivers, can be extended for other drivers"""

    driver_path: str = None

    def __init__(self, driver_path=None, debug_prn: bool = False):
        self.driver_path = driver_path

        assert self.is_chrome_in_path()

        if debug_prn:
            print(self.get_chromedriver_version())

    def is_chrome_in_path(self):
        result = subprocess.run(["which", "google-chrome"], stdout=subprocess.PIPE)
        is_chrome_in_path = result.stdout != b""

        if not is_chrome_in_path:
            raise DriverGenerator_NotInPath()

        return is_chrome_in_path

    def get_chromedriver_version(self):
        result = subprocess.run(["chromedriver", "--version"], stdout=subprocess.PIPE)
        return result.stdout.decode("utf-8")

    def get_webdriver(self, driver_path="/usr//bin/chromedriver") -> webdriver:
        """create a chrome webdriver"""

        if not self.driver_path:
            self.driver_path = driver_path

        chrome_options = webdriver.ChromeOptions()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")  # needed for LINUX environments
        chrome_options.add_argument(self.driver_path or driver_path)

        return webdriver.Chrome(chrome_options)

# %% ../../nbs/scraper/driver.ipynb 10
def get_pagesource(
    url: str,
    driver: webdriver,
    search_criteria_tuple=None,  # for dynamically rendered pages, pass a WebDriverWait search tuple (search_element, element name)
    max_sleep_time=15,
    return_soup: bool = True,
    debug_prn: bool = False,
):
    """retrieve page_source"""
    try:
        if debug_prn:
            print(f"ðŸ’¤ pagesource: retrieving {url} ðŸ’¤")

        driver.get(url)

        if search_criteria_tuple:
            WebDriverWait(driver, timeout=max_sleep_time).until(
                EC.presence_of_element_located(search_criteria_tuple)
            )

        if return_soup:
            return BeautifulSoup(driver.page_source, "lxml")

        driver.close()

        return driver.page_source

    except Exception as e:
        print(
            f"ERROR: {e} -  {url} failed to load page within {max_sleep_time} seconds.  is the element represented in the element list?"
        )
