# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/scraper/scraper.ipynb.

# %% ../../nbs/scraper/scraper.ipynb 2
import re

import os
from dataclasses import dataclass
from typing import List, Callable, Any
from bs4 import BeautifulSoup
from __future__ import annotations

from urllib.parse import urljoin, urlparse

from concurrent.futures import ThreadPoolExecutor


from urllib.parse import urlparse
from gdoc_sync.utils import upsert_folder, convert_str_file_name

import gdoc_sync.scraper.driver as dg

# %% auto 0
__all__ = ['DomoKB_ScrapeConfig_Article', 'DomoKB_ScrapeConfig_Topic', 'DomoKB_ScrapeConfig_KnowledgeBase',
           'DomoKB_ScrapeFactory', 'extract_links', 'generate_filename_from_url', 'Scrape_Config',
           'ScrapeTask_NoDriverProvided', 'ScrapeTask_NoBaseUrl', 'ScrapeTask_NoScrapeCrawler', 'Scrape_Task',
           'Scrape_Factory_NoConfigMatch', 'Scrape_Factory', 'domokb_article_content_extractor_fn',
           'domokb_topic_content_extractor_fn', 'domokb_knowledgebase_content_extractor_fn', 'Scrape_Crawler']

# %% ../../nbs/scraper/scraper.ipynb 3
from nbdev.showdoc import patch_to

# %% ../../nbs/scraper/scraper.ipynb 9
def extract_links(
    soup: BeautifulSoup,
    base_url: str = None,
    custom_link_extractor_fn: Callable = None,
    debug_prn: bool = False,
) -> [str]:
    """returns a list of urls"""

    links_ls = []

    for link in soup.findAll("a"):
        if not link.has_attr("href"):
            continue

        url = link["href"]

        if debug_prn:
            print(url)

        if url.startswith("/") and base_url:
            url = urljoin(base_url, url)

        if base_url and not url.startswith(base_url):
            continue

        if custom_link_extractor_fn:
            url = custom_link_extractor_fn(url, base_url)

        if url in links_ls or not url:
            continue

        links_ls.append(url)

    return list(set(links_ls))

# %% ../../nbs/scraper/scraper.ipynb 11
def generate_filename_from_url(url, base_folder=None, file_name=None) -> str:
    parsed_url = urlparse(url)

    base_str = "_".join([str for str in parsed_url[2].split("/") if str])

    if base_folder:
        base_str = os.path.join(base_folder, base_str)

    if file_name:
        base_str = os.path.join(base_str, file_name)

    return base_str

# %% ../../nbs/scraper/scraper.ipynb 14
@dataclass
class Scrape_Config:
    """class for collating data about how to scrape a page"""

    pattern: re.Pattern  # url pattern
    content_extractor_fn: Callable = None
    link_extractor_fn: Callable = extract_links

    search_element_type: Any = None
    search_element_text: str = None

    def __id__(self, other):
        return self.pattern == other.pattern

    def get_search_tuple(self) -> Any:
        if not self.search_element_text and self.search_element_type:
            return None

        return (self.search_element_type, self.search_element_text)

    def is_text_match_pattern(self, text, debug_prn: bool = False):
        pattern = re.compile(self.pattern)

        match_pattern = pattern.match(text)

        if debug_prn:
            print({"text": text, "pattern": self.pattern})

        if not match_pattern:
            return False

        if debug_prn:
            print(match_pattern)

        return True

# %% ../../nbs/scraper/scraper.ipynb 15
class ScrapeTask_NoDriverProvided(Exception):
    def __init__(self):
        super().__init__("Driver not provided")


class ScrapeTask_NoBaseUrl(Exception):
    def __init__(self):
        super().__init__(
            "no base_url provided to scrape_task. must run as is_test = True"
        )


class ScrapeTask_NoScrapeCrawler(Exception):
    def __init__(self):
        super().__init__(
            "no scrape_crawler provided.  must run scrape_page as is_test = True"
        )


@dataclass
class Scrape_Task:
    url: str  # will be assigned after retrieved from scrape_factory

    base_url: str = None
    download_folder: str = None

    generate_filename_fn: Callable = generate_filename_from_url

    max_sleep_time: int = 10
    scrape_config: Scrape_Config = None  # to define custom ways of extracting content
    scrape_crawler: Scrape_Crawler = None  # will be optional parent threadpool manager.

    @classmethod
    def _from_factory(
        cls,
        url,
        base_url,
        scrape_config,
        scrape_crawler=None,
        download_folder=None,
    ):
        return cls(
            url=url,
            base_url=base_url,
            download_folder=download_folder,
            scrape_config=scrape_config,
            scrape_crawler=scrape_crawler,
        )

    def _get_pagesource(self, driver=None, debug_prn: bool = False):
        """
        handles fetching the pagesource, html content of a URL
        """

        url = self.url

        driver = (
            driver
            or (
                self.scrape_crawler
                and self.scrape_crawler.driver_generator.get_driver()
            )
            or None
        )

        if not driver:
            raise ScrapeTask_NoDriverProvided()

        search_criteria_tuple = (
            self.scrape_config and self.scrape_config.get_search_tuple()
        ) or None

        max_sleep_time = self.max_sleep_time or 15

        if debug_prn:
            print(
                {
                    "url": url,
                    "max_sleep_time": max_sleep_time,
                    "search_criteria": search_criteria_tuple,
                }
            )

        return dg.get_pagesource(
            url=url,
            search_criteria_tuple=search_criteria_tuple,
            driver=driver,
            max_sleep_time=max_sleep_time,
        )

    def _download_content(self, file_name, content):
        dir_name = os.path.dirname(file_name)

        if dir_name[-1] != "/":
            dir_name += "/"

        upsert_folder(dir_name)

        with open(file_name, "w", encoding="utf-8") as f:
            f.write(str(content))

        return file_name

# %% ../../nbs/scraper/scraper.ipynb 16
@patch_to(Scrape_Task)
def scrape_page(
    self: Scrape_Task,
    base_url=None,
    driver=None,
    download_folder=None,
    debug_prn: bool = False,
    is_suppress_errors: bool = False,
    is_test: bool = False,
):
    url = self.url

    download_folder = download_folder or self.download_folder or "./SCRAPE"

    scrape_crawler = self.scrape_crawler

    driver = (
        driver
        or (scrape_crawler and scrape_crawler.driver_generator.get_webdriver())
        or None
    )

    if not driver:
        raise ScrapeTask_NoDriverProvided()

    base_url = (
        base_url
        or self.base_url
        or (scrape_crawler and scrape_crawler.base_url)
        or None
    )

    if debug_prn:
        print(f"scraping_page {url}")

    try:
        soup = self._get_pagesource(driver=driver, debug_prn=debug_prn)

        file_name = self.generate_filename_fn(
            url=url, base_folder=download_folder, file_name="index.html"
        )

        save_location = self._download_content(file_name=file_name, content=soup)

        # Extract the article content
        if self.scrape_config:
            content = self.scrape_config.content_extractor_fn(soup)

            content_name = self.generate_filename_fn(
                url=url, base_folder=download_folder, file_name="content.html"
            )

            self._download_content(
                content_name,
                content=content,
            )

        print(f"ðŸŽ‰ successfully scraped {url} to {save_location}")

        if is_test:
            return []

        if not base_url:
            raise ScrapeTask_NoBaseUrl()

        if not scrape_crawler:
            raise ScrapeTask_NoScrapeCrawler()

        """if config is part of a scrape_crawler / managed threadpool will update crawler's urls_to_visit list."""

        urls_to_visit = self.scrape_config.link_extractor_fn(soup, base_url)

        scrape_crawler._add_url_to_visited(url)

        for url in urls_to_visit:
            scrape_crawler._add_url_to_visit(url)

        return urls_to_visit

    except Exception as e:
        if not is_suppress_errors:
            raise (e)
        return f"ðŸ’€ failed to download {url} received errror{e}"

# %% ../../nbs/scraper/scraper.ipynb 20
class Scrape_Factory_NoConfigMatch(Exception):
    def __init__(self, text):
        super().__init__(
            f"{text} has no pattern match in factory_configs, add an appropriate config or check pattern matches"
        )


@dataclass
class Scrape_Factory:
    """class handles a list of Scrape_Configs and will return the 'correct one' given a URL"""

    factory_configs: List[Scrape_Config]

    def get_task(
        self,
        url,
        download_folder="./SCRAPE",
        base_url=None,
        scrape_crawler=None,
        debug_prn: bool = False,
    ):
        config = next(
            (
                config
                for config in self.factory_configs
                if config.is_text_match_pattern(url, debug_prn=debug_prn)
            ),
            None,
        )

        if not config:
            raise Scrape_Factory_NoConfigMatch(text=url)

        return Scrape_Task._from_factory(
            scrape_config=config,
            url=url,
            base_url=base_url,
            download_folder=download_folder,
            scrape_crawler=scrape_crawler,
        )

# %% ../../nbs/scraper/scraper.ipynb 22
def process_domo_kb_link(url, base_url):
    # remove query params
    url = remove_query_params_from_url(url)

    if not url or not "/s/" not in url:
        return None

    # only keep the first 6 pieces of the URL
    url = "/".join(url.split("/")[:6])

    if url.endswith("/"):
        url = url[:-1]

    return url


def domokb_link_extractor_fn(soup, base_url, debug_prn: bool = False):
    """custom link extractor for processing Domo KBs.
    will be embedded into all DomoKB_ScrapeConfigs
    """
    return extract_links(
        soup,
        custom_link_extractor_fn=process_domo_kb_link,
        base_url=base_url,
        debug_prn=debug_prn,
    )

# %% ../../nbs/scraper/scraper.ipynb 25
def domokb_article_content_extractor_fn(soup) -> BeautifulSoup:
    return soup.find(class_=["article-column"])


DomoKB_ScrapeConfig_Article = Scrape_Config(
    pattern=r".*/s/article/.*",
    link_extractor_fn=domokb_link_extractor_fn,
    content_extractor_fn=domokb_article_content_extractor_fn,
    search_element_type=By.CLASS_NAME,
    search_element_text="slds-form-element",
)

# %% ../../nbs/scraper/scraper.ipynb 27
def domokb_topic_content_extractor_fn(soup) -> BeautifulSoup:
    return soup.find(class_=["knowledge-base"])


DomoKB_ScrapeConfig_Topic = Scrape_Config(
    pattern=r".*/s/topic/.*",
    link_extractor_fn=domokb_link_extractor_fn,
    content_extractor_fn=domokb_topic_content_extractor_fn,
    search_element_type=By.CSS_SELECTOR,
    search_element_text=f".{', .'.join(['section-list-item', 'article-list-item'] )}",
)

# %% ../../nbs/scraper/scraper.ipynb 29
def domokb_knowledgebase_content_extractor_fn(soup) -> BeautifulSoup:
    return soup.find(class_=["knowledge-base"])


DomoKB_ScrapeConfig_KnowledgeBase = Scrape_Config(
    pattern=r".*/s/knowledge-base.*",
    link_extractor_fn=domokb_link_extractor_fn,
    content_extractor_fn=domokb_knowledgebase_content_extractor_fn,
    search_element_type=By.CSS_SELECTOR,
    search_element_text=f".{', .'.join(['topic-nav-container', 'cDomoKBCategoryNav'] )}",
)

# %% ../../nbs/scraper/scraper.ipynb 31
DomoKB_ScrapeFactory = Scrape_Factory(
    [
        DomoKB_ScrapeConfig_Article,
        DomoKB_ScrapeConfig_Topic,
        DomoKB_ScrapeConfig_KnowledgeBase,
    ]
)

# %% ../../nbs/scraper/scraper.ipynb 37
@dataclass
class Scrape_Crawler:
    """threadpool manager for crawling through a list of urls"""

    executor: ThreadPoolExecutor
    scrape_factory: Scrape_Factory
    base_url: str

    download_folder: str
    visited_urls: set
    urls_to_visit: set

    driver_generator: dg.DriverGenerator = None

    def __init__(
        self,
        driver_path,
        scrape_factory: Scrape_Factory,
        urls_to_visit: list,
        base_url: str,
        urls_visited: list = None,
        max_workers=5,
        download_folder: str = "./SCRAPE/",
    ):
        self.base_url = base_url
        self.scrape_factory = scrape_factory
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.driver_generator = dg.DriverGenerator(driver_path=driver_path)
        self.download_folder = download_folder

        self.visited_urls = set()
        if urls_visited:
            [self._add_url_to_visited(url) for url in urls_visited]

        self.urls_to_visit = set()

        if urls_to_visit:
            [self._add_url_to_visit(url) for url in urls_to_visit]

    def _add_url_to_visit(self, url, debug_prn: bool = False):
        """adds a URL to the list of URLS to visit after testing that the URL has not already been visited"""

        if url not in self.visited_urls and url not in self.urls_to_visit:
            if debug_prn:
                print(f"adding {url} to to_vist list")

            self.urls_to_visit.add(url)
            return self.urls_to_visit

    def _add_url_to_visited(self, url, debug_prn: bool = False):
        if url not in self.visited_urls:
            if debug_prn:
                print(f"adding {url} to visited list")

            self.visited_urls.add(url)
            return self.visited_urls

    def _quit(self):
        """call when the executor queue is empty"""

        self.executor.shutdown(wait=True)
        return f"Done scraping {len(self.visited_urls)} urls"

# %% ../../nbs/scraper/scraper.ipynb 39
@patch_to(Scrape_Crawler)
def crawl_urls(self: Scrape_Crawler, is_test: bool = False, debug_prn: bool = False):
    while self.urls_to_visit:
        url = self.urls_to_visit.pop()
        self.visited_urls.add(url)

        driver = self.driver_generator.get_webdriver()

        task = self.scrape_factory.get_task(
            url, debug_prn=debug_prn, scrape_crawler=self
        )

        future = self.executor.submit(task.scrape_page, driver=driver, is_test=is_test)
        try:
            future.result()

        except Exception as e:
            print(e)

    return self._quit()
