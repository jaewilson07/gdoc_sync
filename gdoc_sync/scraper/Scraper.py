# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/scraper/scraper.ipynb.

# %% ../../nbs/scraper/scraper.ipynb 2
from __future__ import annotations

import os

from dataclasses import dataclass
from typing import List, Callable, Any
from bs4 import BeautifulSoup

import re
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor

from gdoc_sync.utils import upsert_folder, convert_str_file_name
import gdoc_sync.scraper.driver as dg

from nbdev.showdoc import patch_to, show_doc

# %% auto 0
__all__ = ['DomoKB_ScrapeConfig_Article', 'DomoKB_ScrapeConfig_Topic', 'DomoKB_ScrapeConfig_KnowledgeBase',
           'DomoKB_ScrapeFactory', 'remove_query_params_from_url', 'extract_links', 'generate_filename_from_url',
           'Scrape_Config', 'ScrapeTask_NoDriverProvided', 'ScrapeTask_NoBaseUrl', 'ScrapeTask_NoScrapeCrawler',
           'Scrape_Task', 'Scrape_Factory_NoConfigMatch', 'Scrape_Factory', 'domokb_article_content_extractor_fn',
           'domokb_topic_content_extractor_fn', 'domokb_knowledgebase_content_extractor_fn', 'Scrape_Crawler']

# %% ../../nbs/scraper/scraper.ipynb 6
def remove_query_params_from_url(url):
    u = urlparse(url)
    return urljoin(url, urlparse(url).path)

# %% ../../nbs/scraper/scraper.ipynb 8
def extract_links(
    soup: BeautifulSoup,
    base_url: str = None,
    custom_link_extractor_fn: Callable = None,  # can add custom function for handling URLs
    debug_prn: bool = False,
) -> [str]:
    """returns a list of urls"""

    links_ls = []

    for link in soup.findAll("a"):
        if not link.has_attr("href"):
            continue

        url = link["href"]

        if debug_prn:
            print(url)

        if url.startswith("/") and base_url:
            url = urljoin(base_url, url)

        if base_url and not url.startswith(base_url):
            continue

        if custom_link_extractor_fn:
            url = custom_link_extractor_fn(url, base_url)

        if url in links_ls or not url:
            continue

        links_ls.append(url)

    return list(set(links_ls))

# %% ../../nbs/scraper/scraper.ipynb 10
def generate_filename_from_url(url, download_folder=None, file_name=None) -> str:
    parsed_url = urlparse(url)

    file_path = "_".join([str for str in parsed_url[2].split("/") if str])

    if download_folder:
        file_path = os.path.join(download_folder, file_path)

    if file_name:
        file_path = os.path.join(file_path, file_name)

    return file_path

# %% ../../nbs/scraper/scraper.ipynb 13
@dataclass
class Scrape_Config:
    """class for collating data about how to scrape a page
    pass a list of scrape_config into ScrapeFactory
    """

    pattern: re.Pattern  # pattern for matching URLs to appropriate config
    content_extractor_fn: Callable = None  # function for the subset of HTML to extract
    link_extractor_fn: Callable = (
        extract_links  # function for extracting links from soup
    )

    search_element_type: Any = None  # from selenium.webdriver.common.by import By
    search_element_text: str = None

    def get_search_tuple(self) -> Any:
        """used by dg.get_pagesource() function to wait if rendered page has rendered"""

        if not self.search_element_text and self.search_element_type:
            return None

        return (self.search_element_type, self.search_element_text)

    def is_text_match_pattern(self, text, debug_prn: bool = False):
        pattern = re.compile(self.pattern)

        match_pattern = pattern.match(text)

        if debug_prn:
            print({"text": text, "pattern": self.pattern})

        if not match_pattern:
            return False

        if debug_prn:
            print(match_pattern)

        return True

    def __id__(self, other):
        return self.pattern == other.pattern

# %% ../../nbs/scraper/scraper.ipynb 15
class ScrapeTask_NoDriverProvided(Exception):
    def __init__(self):
        super().__init__("Driver not provided")


class ScrapeTask_NoBaseUrl(Exception):
    def __init__(self):
        super().__init__(
            "no base_url provided to scrape_task. must run as is_test = True"
        )


class ScrapeTask_NoScrapeCrawler(Exception):
    def __init__(self):
        super().__init__(
            "no scrape_crawler provided.  must run scrape_page as is_test = True"
        )


@dataclass
class Scrape_Task:
    """scrape_factory matches a URL to a scrape_config and returns a scrape_task"""

    url: str  # will be assigned after retrieved from scrape_factory

    base_url: str = None
    download_folder: str = None

    generate_filename_fn: Callable = generate_filename_from_url

    max_sleep_time: int = 10
    scrape_config: Scrape_Config = None
    scrape_crawler: Scrape_Crawler = None  # will be optional parent threadpool manager.

    @classmethod
    def _from_factory(
        cls,
        url,
        base_url,
        scrape_config,
        scrape_crawler=None,
        download_folder=None,
    ):
        return cls(
            url=url,
            base_url=base_url,
            download_folder=download_folder,
            scrape_config=scrape_config,
            scrape_crawler=scrape_crawler,
        )

    def _get_pagesource(self, driver=None, debug_prn: bool = False):
        """
        handles fetching the pagesource, html content of a URL
        """

        url = self.url

        driver = (
            driver
            or (
                self.scrape_crawler
                and self.scrape_crawler.driver_generator.get_driver()
            )
            or None
        )

        if not driver:
            raise ScrapeTask_NoDriverProvided()

        search_criteria_tuple = (
            self.scrape_config and self.scrape_config.get_search_tuple()
        ) or None

        max_sleep_time = self.max_sleep_time or 15

        if debug_prn:
            print(
                {
                    "url": url,
                    "max_sleep_time": max_sleep_time,
                    "search_criteria": search_criteria_tuple,
                }
            )

        return dg.get_pagesource(
            url=url,
            search_criteria_tuple=search_criteria_tuple,
            driver=driver,
            max_sleep_time=max_sleep_time,
        )

    def _download_content(self, file_name, content):
        dir_name = os.path.dirname(file_name)

        if dir_name[-1] != "/":
            dir_name += "/"

        upsert_folder(dir_name)

        with open(file_name, "w", encoding="utf-8") as f:
            f.write(str(content))

        return file_name

    def _update_crawler(self, soup, base_url, scrape_crawler):
        """if config is part of a scrape_crawler / managed threadpool will update crawler's urls_to_visit list."""

        scrape_crawler._add_url_to_visited(self.url)

        urls_to_visit = self.scrape_config.link_extractor_fn(soup, base_url)

        [scrape_crawler._add_url_to_visit(url) for url in urls_to_visit]

        return urls_to_visit

# %% ../../nbs/scraper/scraper.ipynb 16
@patch_to(Scrape_Task)
def execute(
    self: Scrape_Task,
    base_url=None,
    driver=None,
    download_folder=None,
    debug_prn: bool = False,
    is_suppress_errors: bool = False,
    is_test: bool = False,
):
    """handles executing the scrape_task
    1. get pagesource
    2. download index
    3. download content
    4. update crawler
    - get_links from pagesource
    """
    url = self.url

    download_folder = download_folder or self.download_folder or "./SCRAPE"

    scrape_crawler = self.scrape_crawler

    driver = (
        driver
        or (scrape_crawler and scrape_crawler.driver_generator.get_webdriver())
        or None
    )

    if not driver:
        raise ScrapeTask_NoDriverProvided()

    base_url = (
        base_url
        or self.base_url
        or (scrape_crawler and scrape_crawler.base_url)
        or None
    )

    if debug_prn:
        print(f"scraping_page {url}")

    try:
        soup = self._get_pagesource(driver=driver, debug_prn=debug_prn)

        # download index
        file_name = self.generate_filename_fn(
            url=url, download_folder=download_folder, file_name="index.html"
        )

        save_location = self._download_content(file_name=file_name, content=soup)

        # download content
        if self.scrape_config:
            content = self.scrape_config.content_extractor_fn(soup)

            content_name = self.generate_filename_fn(
                url=url, download_folder=download_folder, file_name="content.html"
            )

            self._download_content(
                content_name,
                content=content,
            )

        print(f"ðŸŽ‰ successfully scraped {url} to {save_location}")

        if is_test:
            return []

        # update crawler
        if not base_url:
            raise ScrapeTask_NoBaseUrl()

        if not scrape_crawler:
            raise ScrapeTask_NoScrapeCrawler()

        """if config is part of a scrape_crawler / managed threadpool will update crawler's urls_to_visit list."""
        return self._update_crawler(
            soup=soup, base_url=base_url, scrape_crawler=scrape_crawler
        )

    except Exception as e:
        if not is_suppress_errors:
            raise (e)
        return f"ðŸ’€ failed to download {url} received errror{e}"

# %% ../../nbs/scraper/scraper.ipynb 20
class Scrape_Factory_NoConfigMatch(Exception):
    def __init__(self, text):
        super().__init__(
            f"{text} has no pattern match in factory_configs, add an appropriate config or check pattern matches"
        )


@dataclass
class Scrape_Factory:
    """class handles a list of Scrape_Configs and will return the 'correct one' given a URL"""

    factory_configs: List[Scrape_Config]

    def get_task(
        self,
        url,
        download_folder="./SCRAPE",
        base_url=None,
        scrape_crawler=None,
        debug_prn: bool = False,
    ):
        config = next(
            (
                config
                for config in self.factory_configs
                if config.is_text_match_pattern(url, debug_prn=debug_prn)
            ),
            None,
        )

        if not config:
            raise Scrape_Factory_NoConfigMatch(text=url)

        return Scrape_Task._from_factory(
            scrape_config=config,
            url=url,
            base_url=base_url,
            download_folder=download_folder,
            scrape_crawler=scrape_crawler,
        )

# %% ../../nbs/scraper/scraper.ipynb 22
def process_domo_kb_link(url, base_url):
    # remove query params
    url = remove_query_params_from_url(url)

    if not url or not "/s/" in url:
        return None

    # only keep the first 6 pieces of the URL
    url = "/".join(url.split("/")[:6])

    if url.endswith("/"):
        url = url[:-1]

    return url


def domokb_link_extractor_fn(soup, base_url, debug_prn: bool = False):
    """custom link extractor for processing Domo KBs.
    will be embedded into all DomoKB_ScrapeConfigs
    """
    return extract_links(
        soup,
        custom_link_extractor_fn=process_domo_kb_link,
        base_url=base_url,
        debug_prn=debug_prn,
    )

# %% ../../nbs/scraper/scraper.ipynb 25
def domokb_article_content_extractor_fn(soup) -> BeautifulSoup:
    return soup.find(class_=["article-column"])


DomoKB_ScrapeConfig_Article = Scrape_Config(
    pattern=r".*/s/article/.*",
    link_extractor_fn=domokb_link_extractor_fn,
    content_extractor_fn=domokb_article_content_extractor_fn,
    search_element_type=By.CLASS_NAME,
    search_element_text="slds-form-element",
)

# %% ../../nbs/scraper/scraper.ipynb 27
def domokb_topic_content_extractor_fn(soup) -> BeautifulSoup:
    return soup.find(class_=["knowledge-base"])


DomoKB_ScrapeConfig_Topic = Scrape_Config(
    pattern=r".*/s/topic/.*",
    link_extractor_fn=domokb_link_extractor_fn,
    content_extractor_fn=domokb_topic_content_extractor_fn,
    search_element_type=By.CSS_SELECTOR,
    search_element_text=f".{', .'.join(['section-list-item', 'article-list-item'] )}",
)

# %% ../../nbs/scraper/scraper.ipynb 29
def domokb_knowledgebase_content_extractor_fn(soup) -> BeautifulSoup:
    return soup.find(class_=["knowledge-base"])


DomoKB_ScrapeConfig_KnowledgeBase = Scrape_Config(
    pattern=r".*/s/knowledge-base.*",
    link_extractor_fn=domokb_link_extractor_fn,
    content_extractor_fn=domokb_knowledgebase_content_extractor_fn,
    search_element_type=By.CSS_SELECTOR,
    search_element_text=f".{', .'.join(['topic-nav-container', 'cDomoKBCategoryNav'] )}",
)

# %% ../../nbs/scraper/scraper.ipynb 31
DomoKB_ScrapeFactory = Scrape_Factory(
    [
        DomoKB_ScrapeConfig_Article,
        DomoKB_ScrapeConfig_Topic,
        DomoKB_ScrapeConfig_KnowledgeBase,
    ]
)

# %% ../../nbs/scraper/scraper.ipynb 37
@dataclass
class Scrape_Crawler:
    """threadpool manager for crawling through a list of urls"""

    executor: ThreadPoolExecutor
    scrape_factory: Scrape_Factory
    base_url: str

    download_folder: str
    visited_urls: set
    urls_to_visit: set

    driver_generator: dg.DriverGenerator = None

    def __init__(
        self,
        driver_path,
        scrape_factory: Scrape_Factory,
        urls_to_visit: list,
        base_url: str,
        urls_visited: list = None,
        max_workers=5,
        download_folder: str = "./SCRAPE/",
    ):
        self.base_url = base_url
        self.scrape_factory = scrape_factory
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.driver_generator = dg.DriverGenerator(driver_path=driver_path)
        self.download_folder = download_folder

        self.visited_urls = set()
        if urls_visited:
            [self._add_url_to_visited(url) for url in urls_visited]

        self.urls_to_visit = set()

        if urls_to_visit:
            [self._add_url_to_visit(url) for url in urls_to_visit]

    def _add_url_to_visit(self, url, debug_prn: bool = False):
        """adds a URL to the list of URLS to visit after testing that the URL has not already been visited"""

        if url not in self.visited_urls and url not in self.urls_to_visit:
            if debug_prn:
                print(f"adding {url} to to_vist list")

            self.urls_to_visit.add(url)
            return self.urls_to_visit

    def _add_url_to_visited(self, url, debug_prn: bool = False):
        if url not in self.visited_urls:
            if debug_prn:
                print(f"adding {url} to visited list")

            self.visited_urls.add(url)
            return self.visited_urls

    def _quit(self):
        """call when the executor queue is empty"""

        self.executor.shutdown(wait=True)
        return f"Done scraping {len(self.visited_urls)} urls"

# %% ../../nbs/scraper/scraper.ipynb 39
@patch_to(Scrape_Crawler)
def crawl_urls(self: Scrape_Crawler, is_test: bool = False, debug_prn: bool = False):
    while self.urls_to_visit:
        url = self.urls_to_visit.pop()
        self.visited_urls.add(url)

        driver = self.driver_generator.get_webdriver()

        task = self.scrape_factory.get_task(
            url, debug_prn=debug_prn, scrape_crawler=self
        )

        future = self.executor.submit(task.execute, driver=driver, is_test=is_test)
        try:
            future.result()

        except Exception as e:
            print(e)

    return self._quit()
