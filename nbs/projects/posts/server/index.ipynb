{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Downloading llama_index-0.9.26-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (0.6.3)\n",
      "Collecting deprecated>=1.2.9.3 (from llama_index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (2023.12.2)\n",
      "Requirement already satisfied: httpx in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (0.26.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (1.5.8)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama_index)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: numpy in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (1.6.1)\n",
      "Requirement already satisfied: pandas in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (8.2.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama_index)\n",
      "  Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from llama_index) (0.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama_index) (2.5)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.9.3->llama_index)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama_index)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama_index) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama_index) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama_index) (1.3.0)\n",
      "Requirement already satisfied: certifi in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from httpx->llama_index) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from httpx->llama_index) (1.0.2)\n",
      "Requirement already satisfied: idna in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from httpx->llama_index) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama_index) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama_index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama_index) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from dataclasses-json->llama_index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from pandas->llama_index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from pandas->llama_index) (2023.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama_index) (2.14.6)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/c/users/jaewi/GitHub/gdoc_sync/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama_index) (1.16.0)\n",
      "Downloading llama_index-0.9.26-py3-none-any.whl (15.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: wrapt, joblib, tiktoken, nltk, deprecated, llama_index\n",
      "Successfully installed deprecated-1.2.14 joblib-1.3.2 llama_index-0.9.26 nltk-3.8.1 tiktoken-0.5.2 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install huggingface_hub\n",
    "# %pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from gdoc_sync.utils import upsert_folder\n",
    "from nbdev.showdoc import patch_to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPath:\n",
    "    install_path: str\n",
    "    cache_path : str = None\n",
    "    embedding_path : str = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.cache_path = self.cache_path or f\"{self.install_path}/cache/\"\n",
    "        self.embedding_path = self.embedding_path or f\"{self.install_path}/embedding/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'install_path': '../../models',\n",
       " 'cache_path': '../../models/cache/',\n",
       " 'embedding_path': '../../models/embedding/'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdict(ModelPath(install_path = \"../../models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "      \n",
    "  install_path : str\n",
    "\n",
    "  prompt_style : str= \"llama2\"\n",
    "  llm_hf_repo_id  : str= \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "  llm_tokenizer_name : str = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "  llm_hf_model_file :str = \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "  embedding_hf_model_name :str= \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "\n",
    "  model_paths : ModelPath = None\n",
    "\n",
    "  def __post_init__(self):\n",
    "    self.model_paths = ModelPath(install_path= self.install_path )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@patch_to(ModelConfig)\n",
    "def setup_model_folders(self, debug_prn: bool = False):\n",
    "    \n",
    "    [upsert_folder(folder_path, debug_prn = debug_prn) for folder_path in asdict(self.model_paths).values()]\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upsert_folder': '/mnt/c/users/jaewi/GitHub/gdoc_sync/nbs/projects/models', 'is_exist': True}\n",
      "{'upsert_folder': '/mnt/c/users/jaewi/GitHub/gdoc_sync/nbs/projects/models/cache', 'is_exist': True}\n",
      "{'upsert_folder': '/mnt/c/users/jaewi/GitHub/gdoc_sync/nbs/projects/models/embedding', 'is_exist': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ModelConfig(install_path='../../models/')\n",
    "config.setup_model_folders( debug_prn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@patch_to(ModelConfig)\n",
    "def download_embeddings_model(self):\n",
    "    print(f\"Downloading embedding {self.embedding_hf_model_name}\")\n",
    "    \n",
    "    snapshot_download(\n",
    "        repo_id=self.embedding_hf_model_name,\n",
    "        cache_dir=self.model_paths.cache_path,\n",
    "        local_dir=self.model_paths.embedding_path,\n",
    "    )\n",
    "    print(f\"Embedding model downloaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embedding BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1987f014dd524e3e98724c4194db9905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model downloaded\n"
     ]
    }
   ],
   "source": [
    "config.download_embeddings_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@patch_to(ModelConfig)\n",
    "def download_llm_model(self, resume_download : bool = True):\n",
    "    \"\"\"Download LLM and create a symlink to the model file\"\"\"\n",
    "    print(f\"Downloading LLM {self.llm_hf_model_file}\")\n",
    "    \n",
    "    hf_hub_download(\n",
    "        repo_id=self.llm_hf_repo_id,\n",
    "        filename=self.llm_hf_model_file,\n",
    "        cache_dir=self.model_paths.cache_path,\n",
    "        local_dir=self.model_paths.install_path,\n",
    "        resume_download=resume_download,\n",
    "    )\n",
    "    print(\"LLM model downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LLM mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "LLM model downloaded!\n"
     ]
    }
   ],
   "source": [
    "config.download_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@patch_to(ModelConfig)\n",
    "def download_tokenizer(self):\n",
    "    print(f\"Downloading tokenizer {self.llm_tokenizer_name}\")\n",
    "    \n",
    "    AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=self.llm_tokenizer_name,\n",
    "        cache_dir=self.model_paths.cache_path\n",
    "    )\n",
    "    print(\"Tokenizer downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer mistralai/Mistral-7B-Instruct-v0.1\n",
      "Tokenizer downloaded!\n"
     ]
    }
   ],
   "source": [
    "config.download_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@patch_to(ModelConfig)\n",
    "def setup(self):\n",
    "    self.download_embeddings_model()\n",
    "    self.download_llm_model()\n",
    "    self.download_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embedding BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cce28f53844f06bec02101d5290c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model downloaded\n",
      "Downloading LLM mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "LLM model downloaded!\n",
      "Downloading tokenizer mistralai/Mistral-7B-Instruct-v0.1\n",
      "Tokenizer downloaded!\n"
     ]
    }
   ],
   "source": [
    "config.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "\n",
    "class EmbeddingComponent:\n",
    "    embedding_model: BaseEmbedding\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig) -> None:\n",
    "\n",
    "        from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "        self.embedding_model = HuggingFaceEmbedding(\n",
    "            model_name=model_config.embedding_hf_model_name,\n",
    "            cache_folder=str(model_config.model_paths.cache_path),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
