{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: llm/qna.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp llm/qna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"http://172.20.2.239:1234/v1/\",\n",
    "    openai_api_key=\"not-needed\",\n",
    "    model=\"local_model\",\n",
    "    # callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def generate_promptchain_for_string(system_prompt, llm) -> ConversationChain:\n",
    "    parser = StrOutputParser()\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export  \n",
    "\n",
    "from typing import Any\n",
    "from queue import Queue, Empty\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def handle_streaming_ref(stream):\n",
    "    response = []\n",
    "    for chunk in stream:\n",
    "        print(chunk, end=\"|\", flush=True)\n",
    "        response.append(chunk.content)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        response = \"\".join(response).strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "def get_llm_response(\n",
    "    user_content,\n",
    "    llm,\n",
    "    chat_history = None,\n",
    "    system_prompt=\"You are a QandA Bot\",\n",
    "    temperature = 0.7\n",
    ") ->AIMessage:\n",
    "    \n",
    "    chat_history = chat_history or ChatMessageHistory()\n",
    "    chat_history.add_user_message(user_content)\n",
    "\n",
    "    chain = generate_promptchain_for_string(system_prompt=system_prompt, llm = llm )\n",
    "\n",
    "    def task():\n",
    "        response = llm(chain.stream({\"messages\": chat_history.messages}, temperature = temperature))\n",
    "        q.put(job_done)\n",
    "    \n",
    "    t = Thread(target=task)\n",
    "    t.start()\n",
    "    \n",
    "\n",
    "    chat_history.add_ai_message(response)\n",
    "    \n",
    "q = Queue()\n",
    "job_done = object()\n",
    "\n",
    "class QueueCallback(BaseCallbackHandler):\n",
    "    \"\"\"Callback handler for streaming LLM responses to a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        self.q.put(token)\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs: Any) -> None:\n",
    "        return self.q.empty()\n",
    "    \n",
    "callbacks = [StreamingStdOutCallbackHandler(), QueueCallback(q)]\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"http://172.20.2.239:1234/v1/\",\n",
    "    openai_api_key=\"not-needed\",\n",
    "    model=\"local_model\",\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object RunnableSequence.stream at 0x7f97cb9216c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_response(\n",
    "    user_content =\"Translate this sentence from English to French: I love programming.\",\n",
    "    system_prompt=\"You are a translator.  Be very succinct\", \n",
    "    llm = llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MemoryManager:\n",
    "    system_prompt: str\n",
    "    chat_history = None\n",
    "    llm\n",
    "\n",
    "    def __init__(self, system_prompt, llm):\n",
    "        self.llm = llm\n",
    "        self.system_prompt = system_prompt\n",
    "        \n",
    "        self.chat_history = ChatMessageHistory()   \n",
    "\n",
    "    def add_user_message(self, content):\n",
    "\n",
    "        self.chat_history.add_user_message(content)\n",
    "        self._get_ai_streaming_response()\n",
    "\n",
    "    def _generate_promptchain(self) -> ConversationChain:\n",
    "        return generate_promptchain(system_prompt=self.system_prompt, llm = self.llm)\n",
    "\n",
    "    \n",
    "    def _get_ai_streaming_response(self):\n",
    "\n",
    "        chain = self._generate_promptchain()\n",
    "\n",
    "        response = []\n",
    "        for chunk in chain.stream({\"messages\":self.chat_history.messages}):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            response.append(chunk.content)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        response = \"\".join(response).strip()\n",
    "\n",
    "        self.chat_history.add_ai_message(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "mm = MemoryManager(system_prompt=\"You are a translator.  Be very succinct\", llm = llm)\n",
    "\n",
    "print(mm.add_user_message(\"Translate this sentence from English to French: I love programming.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! In French, the sentence \"I love programming\" can be translated as \"Je préfère programmer\" or \"J'aime programmer\". Both mean \"I prefer to program\" or \"I like to program\".\n"
     ]
    }
   ],
   "source": [
    "print(mm.add_user_message(\"repeat what you just said?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to python llm_qna.ipynb --output ./_test/llm.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
