{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: scraper.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp scraper.Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "import re\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Callable, Any\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from gdoc_sync.utils import upsert_folder, convert_str_file_name\n",
    "\n",
    "import gdoc_sync.scraper.driver as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "from nbdev.showdoc import patch_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from selenium.webdriver.common.by import By\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromeDriver 120.0.6099.109 (3419140ab665596f21b385ce136419fde0924272-refs/branch-heads/6099@{#1483})\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<html dir=\"ltr\" lang=\"en-US\"><head><title>Archived Feature Release Notes</title><meta content=\"defau'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from selenium.webdriver.common.by import By\n",
    "\n",
    "test_urls = [\n",
    "    \"https://domo-support.domo.com/s/article/36004740075?language=en_US\",\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US\",  # list of articles\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US\",  # list of topics\n",
    "    \"https://domo-support.domo.com/s/knowledge-base?language=en_US\",\n",
    "]\n",
    "\n",
    "drivergenerator = dg.DriverGenerator(debug_prn=True)\n",
    "\n",
    "driver = drivergenerator.get_webdriver()\n",
    "\n",
    "test_soup = dg.get_pagesource(\n",
    "    driver=driver,\n",
    "    url=test_urls[3],\n",
    "    search_criteria_tuple=(By.CLASS_NAME, \"topic-nav-container\"),\n",
    "    max_sleep_time=15,\n",
    "    return_soup=True,\n",
    ")\n",
    "\n",
    "test_soup = dg.get_pagesource(\n",
    "    driver=driver,\n",
    "    url=test_urls[2],\n",
    "    search_criteria_tuple=(\n",
    "        By.CSS_SELECTOR,\n",
    "        f\".{', .'.join(['section-list-item', 'article-list-item'] )}\",\n",
    "    ),\n",
    "    max_sleep_time=15,\n",
    "    return_soup=True,\n",
    ")\n",
    "\n",
    "str(test_soup)[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_links(\n",
    "    soup: BeautifulSoup,\n",
    "    base_url: str = None,\n",
    "    custom_link_extractor_fn: Callable = None,\n",
    "    debug_prn: bool = False,\n",
    ") -> [str]:\n",
    "    \"\"\"returns a list of urls\"\"\"\n",
    "\n",
    "    links_ls = []\n",
    "\n",
    "    for link in soup.findAll(\"a\"):\n",
    "        if debug_prn:\n",
    "            print(link)\n",
    "\n",
    "        if link.has_attr(\"href\"):\n",
    "            url = link[\"href\"]\n",
    "\n",
    "            if custom_link_extractor_fn:\n",
    "                url = custom_link_extractor_fn(link, base_url)\n",
    "\n",
    "            if url:\n",
    "                links_ls.append(url)\n",
    "\n",
    "    return list(set(links_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.domo.com',\n",
       " 'https://www.domo.com/start/developer',\n",
       " 'https://innovators.domo.com',\n",
       " 'https://www.domo.com/domo-central/community-awards',\n",
       " 'https://developer.domo.com/login',\n",
       " 'https://www.domo.com/domo-central/community',\n",
       " 'https://learndomo.domo.com/pages/15/learn-by-skill-or-role',\n",
       " 'archived-feature-release-notes?nocache=https%3A%2F%2Fdomo-support.domo.com%2Fs%2Ftopic%2F0TO5w000000Zan7GAC%2Farchived-feature-release-notes%3Flanguage%3Den_US',\n",
       " 'https://www.domo.com/blog/category/domo-community',\n",
       " 'https://www.domo.com/domo-central/help/videos',\n",
       " '/s/topic/0TO5w000000ZamwGAC',\n",
       " 'https://www.domo.com/help-center/get-started',\n",
       " 'https://linkedin.com/company/domo-inc-',\n",
       " 'https://www.domo.com/domo-central/help/data-creators-and-analysts',\n",
       " '/s/topic/0TO5w000000ZlOmGAK',\n",
       " 'https://community-forums.domo.com/main/categories',\n",
       " 'https://community-forums.domo.com/main/categories/welcome',\n",
       " 'https://university.domo.com/training/domo-trains-you',\n",
       " 'https://facebook.com/domoHQ/timeline/',\n",
       " 'https://www.domo.com/domo-central/help/get-started',\n",
       " 'https://community-forums.domo.com/main/events/category',\n",
       " '/s/knowledge-base/',\n",
       " 'https://www.domo.com/domo-central/help/data-consumers',\n",
       " 'https://learndomo.domo.com/pages/13/learning-home',\n",
       " 'https://learndomo.domo.com/pages/16/get-certified',\n",
       " 'https://www.domo.com/domo-central',\n",
       " 'https://instagram.com/domo/',\n",
       " '/s/topic/0TO5w000000ZlOnGAK',\n",
       " 'https://community-forums.domo.com/main/categories/ideas',\n",
       " 'https://domo.com/company/cookies',\n",
       " 'https://community-forums.domo.com/main',\n",
       " 'https://www.domo.com/domo-central/help',\n",
       " 'https://domo-support.domo.com/s/knowledge-base?language=en_US',\n",
       " 'javascript:void(0);',\n",
       " 'https://twitter.com/domotalk',\n",
       " 'https://developer.domo.com',\n",
       " 'https://www.domo.com/domo-central/university',\n",
       " 'https://www.domo.com/login',\n",
       " 'https://www.domo.com/login/customer-community',\n",
       " 'https://domo.com/company/patents',\n",
       " 'https://www.domo.com/domo-for-good',\n",
       " 'https://domo.com/company/privacy-policy',\n",
       " 'https://www.domo.com/domo-central/help/data-engineers',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC/release-notes?language=en_US']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_links(\n",
    "    test_soup,\n",
    "    debug_prn=False,\n",
    "    #   custom_link_extractor_fn= domokb_link_extractor_fn,\n",
    "    #   base_url = \"https://domo-support.domo.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def generate_filename_from_url(url, base_folder=None, file_name=None):\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    base_str = \"_\".join([str for str in parsed_url[2].split(\"/\") if str])\n",
    "\n",
    "    if base_folder:\n",
    "        base_str = os.path.join(base_folder, base_str)\n",
    "\n",
    "    if file_name:\n",
    "        base_str = os.path.join(base_str, file_name)\n",
    "\n",
    "    return base_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s_article_36004740075',\n",
      " 's_topic_0TO5w000000ZlOmGAK_20202023',\n",
      " 's_topic_0TO5w000000Zan7GAC_archived-feature-release-notes',\n",
      " 's_knowledge-base']\n",
      "-- alternative with base_url and different file name\n",
      "['/SCRAPE/s_article_36004740075/index.html',\n",
      " '/SCRAPE/s_topic_0TO5w000000ZlOmGAK_20202023/index.html',\n",
      " '/SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html',\n",
      " '/SCRAPE/s_knowledge-base/index.html']\n"
     ]
    }
   ],
   "source": [
    "pprint([generate_filename_from_url(url) for url in test_urls])\n",
    "\n",
    "print(\"-- alternative with base_url and different file name\")\n",
    "pprint([generate_filename_from_url(url, \"/SCRAPE\", \"index.html\") for url in test_urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class ScrapeConfig_NoDriverProvided(Exception):\n",
    "    def __init__(self, function_name=None):\n",
    "        function_str = \" to {function_name} \" if function_name else \"\"\n",
    "        super().__init__(\"Driver not provided\" + function_str)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Config:\n",
    "    \"\"\"class for collating data about how to scrape a page\"\"\"\n",
    "\n",
    "    pattern: re.Pattern  # url pattern\n",
    "    link_extractor_fn: Callable = extract_links\n",
    "\n",
    "    generate_filename_fn: Callable = generate_filename_from_url\n",
    "    content_extractor_fn: Callable = None\n",
    "\n",
    "    search_element_type: Any = None\n",
    "    search_element_text: str = None\n",
    "\n",
    "    max_sleep_time: int = 10\n",
    "\n",
    "    url = None  # will be assigned after retrieved from scrape_factory\n",
    "    download_folder = \"./SCRAPE\"\n",
    "\n",
    "    scrape_crawler = None  # will be optional parent threadpool manager.\n",
    "\n",
    "    def __id__(self, other):\n",
    "        return self.pattern == other.pattern\n",
    "\n",
    "    def get_search_tuple(self) -> Any:\n",
    "        if not self.search_element_text and self.search_element_type:\n",
    "            return None\n",
    "\n",
    "        return (self.search_element_type, self.search_element_text)\n",
    "\n",
    "    def is_text_match_pattern(self, text, debug_prn: bool = False):\n",
    "        pattern = re.compile(self.pattern)\n",
    "\n",
    "        match_pattern = pattern.match(text)\n",
    "\n",
    "        if debug_prn:\n",
    "            print({\"text\": text, \"pattern\": self.pattern})\n",
    "\n",
    "        if not match_pattern:\n",
    "            return False\n",
    "\n",
    "        if debug_prn:\n",
    "            print(match_pattern)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Config)\n",
    "def _get_pagesource(self: Scrape_Config, url: str, driver, debug_prn: bool = False):\n",
    "    \"\"\"handles fetching the pagesource, html content of a URL\n",
    "    the classes' scrape_factory.get_factory_config() method will identify what to look for for that specific URL\n",
    "    \"\"\"\n",
    "\n",
    "    search_criteria_tuple = self.get_search_tuple()\n",
    "    max_sleep_time = self.max_sleep_time\n",
    "\n",
    "    if debug_prn:\n",
    "        print(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"max_sleep_time\": max_sleep_time,\n",
    "                \"search_criteria\": search_criteria_tuple,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    pagesource = dg.get_pagesource(\n",
    "        url=url,\n",
    "        search_criteria_tuple=search_criteria_tuple,\n",
    "        driver=driver,\n",
    "        max_sleep_time=max_sleep_time,\n",
    "    )\n",
    "\n",
    "    if not pagesource:\n",
    "        raise Exception(f\"unable to retrieve source {url}\")\n",
    "\n",
    "    return pagesource\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Config)\n",
    "def _download_content(self: Scrape_Config, file_name, content):\n",
    "    dir_name = os.path.dirname(file_name)\n",
    "\n",
    "    if dir_name[-1] != \"/\":\n",
    "        dir_name += \"/\"\n",
    "\n",
    "    upsert_folder(dir_name)\n",
    "\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(content))\n",
    "\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Factory converts URL into Scrape_Config task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Scrape_Factory_NoConfigMatch(Exception):\n",
    "    def __init__(self, text):\n",
    "        super().__init__(\n",
    "            f\"{text} has no pattern match in factory_configs, add an appropriate config or check pattern matches\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ScrapeFactory_NoBaseUrl(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            \"no base_url provided to scrape_factory. unable to extract relevant URLs\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Factory:\n",
    "    \"\"\"class handles a list of Scrape_Configs and will return the 'correct one' given a URL\"\"\"\n",
    "\n",
    "    factory_configs: List[Scrape_Config]\n",
    "\n",
    "    def get_factory_config(\n",
    "        self,\n",
    "        url,\n",
    "        base_url=None,\n",
    "        download_folder=\"./SCRAPE\",\n",
    "        scrape_crawler=None,\n",
    "        debug_prn: bool = False,\n",
    "    ):\n",
    "        config = next(\n",
    "            (\n",
    "                config\n",
    "                for config in self.factory_configs\n",
    "                if config.is_text_match_pattern(url, debug_prn=debug_prn)\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if not config:\n",
    "            raise Scrape_Factory_NoConfigMatch(text=url)\n",
    "\n",
    "        config.url = url\n",
    "        config.download_folder = download_folder\n",
    "        config.base_url = (\n",
    "            base_url or (scrape_crawler and scrape_crawler.base_url) or None\n",
    "        )\n",
    "        config.scrape_crawler = scrape_crawler\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Config)\n",
    "def scrape_page(\n",
    "    self: Scrape_Config,\n",
    "    url=None,  # can be passed in or retrieved from class after it has been converted into a task (been returned by scrape_factory)\n",
    "    base_url=None,\n",
    "    driver=None,\n",
    "    debug_prn: bool = False,\n",
    "    is_suppress_errors: bool = False,\n",
    "    is_test: bool = False,\n",
    "):\n",
    "    url = url or self.url\n",
    "\n",
    "    driver = (\n",
    "        driver\n",
    "        or (\n",
    "            self.scrape_crawler\n",
    "            and self.scape_crawler.driver_generator\n",
    "            and self.scrape_crawler.driver_generator.get_webdriver()\n",
    "        )\n",
    "        or None\n",
    "    )\n",
    "\n",
    "    if not driver:\n",
    "        raise ScrapeConfig_NoDriverProvided()\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"scraping_page {url}\")\n",
    "\n",
    "    try:\n",
    "        soup = self._get_pagesource(url=url, driver=driver, debug_prn=debug_prn)\n",
    "\n",
    "        file_name = self.generate_filename_fn(\n",
    "            url=self.url, base_folder=self.download_folder, file_name=\"index.html\"\n",
    "        )\n",
    "\n",
    "        save_location = self._download_content(file_name=file_name, content=soup)\n",
    "\n",
    "        # Extract the article content\n",
    "        if self.content_extractor_fn:\n",
    "            content = self.content_extractor_fn(soup)\n",
    "\n",
    "            content_name = self.generate_filename_fn(\n",
    "                url=self.url, base_folder=self.download_folder, file_name=\"content.html\"\n",
    "            )\n",
    "\n",
    "            self._download_content(\n",
    "                content_name,\n",
    "                content=content,\n",
    "            )\n",
    "\n",
    "        print(f\"ðŸŽ‰ successfully scraped {url} to {save_location}\")\n",
    "\n",
    "        if is_test:\n",
    "            return True\n",
    "\n",
    "        if not is_test:\n",
    "            base_url = (\n",
    "                base_url\n",
    "                or self.base_url\n",
    "                or (self.scrape_crawler and self.scape_crawler.base_url)\n",
    "                or None\n",
    "            )\n",
    "\n",
    "            if not base_url:\n",
    "                raise ScrapeFactory_NoBaseUrl()\n",
    "            \"\"\"if config is part of a scrape_crawler / managed threadpool will update crawler's urls_to_visit list.\"\"\"\n",
    "\n",
    "            return self.link_extractor_fn(soup, base_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        if not is_suppress_errors:\n",
    "            raise (e)\n",
    "        return f\"ðŸ’€ failed to download {url} received errror{e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DomoKB_ScrapeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def process_domo_kb_link(link, base_url):\n",
    "    url = link.get(\"href\")\n",
    "\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    # for relative addresses concat base_url\n",
    "    if url.startswith(\"/s/\"):\n",
    "        url = urljoin(base_url, url)\n",
    "\n",
    "    # ignore urls not orginating from base_url\n",
    "    if not url.startswith(base_url):\n",
    "        return None\n",
    "\n",
    "    # remove query params\n",
    "    url = urljoin(url, urlparse(url).path)\n",
    "\n",
    "    # only keep the first 6 pieces of the URL\n",
    "    url = \"/\".join(url.split(\"/\")[:6])\n",
    "\n",
    "    if url.endswith(\"/\"):\n",
    "        url = url[:-1]\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def domokb_link_extractor_fn(soup, base_url, debug_prn: bool = False):\n",
    "    return extract_links(\n",
    "        soup,\n",
    "        custom_link_extractor_fn=process_domo_kb_link,\n",
    "        base_url=base_url,\n",
    "        debug_prn=debug_prn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK',\n",
       " 'https://domo-support.domo.com/s/knowledge-base',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOnGAK',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domokb_link_extractor_fn(test_soup, \"https://domo-support.domo.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "def domokb_article_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"article-column\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Article = Scrape_Config(\n",
    "    pattern=r\".*/s/article/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_article_content_extractor_fn,\n",
    "    search_element_type=By.CLASS_NAME,\n",
    "    search_element_text=\"slds-form-element\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def domokb_topic_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"knowledge-base\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Topic = Scrape_Config(\n",
    "    pattern=r\".*/s/topic/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_topic_content_extractor_fn,\n",
    "    search_element_type=By.CSS_SELECTOR,\n",
    "    search_element_text=f\".{', .'.join(['section-list-item', 'article-list-item'] )}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_NavContainer\n",
    "cDomoKBCategoryNav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "def domokb_knowledgebase_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"knowledge-base\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_KnowledgeBase = Scrape_Config(\n",
    "    pattern=r\".*/s/knowledge-base.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_knowledgebase_content_extractor_fn,\n",
    "    search_element_type=By.CSS_SELECTOR,\n",
    "    search_element_text=f\".{', .'.join(['topic-nav-container', 'cDomoKBCategoryNav'] )}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeFactory\n",
    "A factory pattern receives a parameter (the url to match to) then returns the appropriate implementation `Scrape_Config` for handling that url.\n",
    "\n",
    "This pattern could be extended for any type of website; however we have implemented a factory specifically for handling Domo Kbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "DomoKB_ScrapeFactory = Scrape_Factory(\n",
    "    [\n",
    "        DomoKB_ScrapeConfig_Article,\n",
    "        DomoKB_ScrapeConfig_Topic,\n",
    "        DomoKB_ScrapeConfig_KnowledgeBase,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation\n",
    "\n",
    "Of retrieving the scrape_config using the `scrape_factory.get_factory_config() method\n",
    "\n",
    "- test_urls is a list of URLS\n",
    "- use a list comprenension to show we can retrieve the correct config for different URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://domo-support.domo.com/s/article/36004740075?language=en_US',\n",
       "  'config': Scrape_Config(pattern='.*/s/article/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f793342b6a0>, generate_filename_fn=<function generate_filename_from_url at 0x7f7932e714e0>, content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f7932e71da0>, search_element_type='class name', search_element_text='slds-form-element', max_sleep_time=10)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US',\n",
       "  'config': Scrape_Config(pattern='.*/s/topic/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f793342b6a0>, generate_filename_fn=<function generate_filename_from_url at 0x7f7932e714e0>, content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f79b67dc5e0>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item', max_sleep_time=10)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US',\n",
       "  'config': Scrape_Config(pattern='.*/s/topic/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f793342b6a0>, generate_filename_fn=<function generate_filename_from_url at 0x7f7932e714e0>, content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f79b67dc5e0>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item', max_sleep_time=10)},\n",
       " {'url': 'https://domo-support.domo.com/s/knowledge-base?language=en_US',\n",
       "  'config': Scrape_Config(pattern='.*/s/knowledge-base.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f793342b6a0>, generate_filename_fn=<function generate_filename_from_url at 0x7f7932e714e0>, content_extractor_fn=<function domokb_knowledgebase_content_extractor_fn at 0x7f7932e70c20>, search_element_type='css selector', search_element_text='.topic-nav-container, .cDomoKBCategoryNav', max_sleep_time=10)}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scrape_tasks = [\n",
    "    {\n",
    "        \"url\": url,\n",
    "        \"config\": DomoKB_ScrapeFactory.get_factory_config(\n",
    "            url, base_url=\"https://domo-support.domo.com\", debug_prn=False\n",
    "        ),\n",
    "    }\n",
    "    for url in test_urls\n",
    "]\n",
    "\n",
    "test_scrape_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pattern': '.*/s/topic/.*',\n",
       " 'link_extractor_fn': <function __main__.domokb_link_extractor_fn(soup, base_url, debug_prn: bool = False)>,\n",
       " 'generate_filename_fn': <function __main__.generate_filename_from_url(url, base_folder=None, file_name=None)>,\n",
       " 'content_extractor_fn': <function __main__.domokb_topic_content_extractor_fn(soup) -> bs4.BeautifulSoup>,\n",
       " 'search_element_type': 'css selector',\n",
       " 'search_element_text': '.section-list-item, .article-list-item',\n",
       " 'max_sleep_time': 10,\n",
       " 'url': 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US',\n",
       " 'download_folder': './SCRAPE',\n",
       " 'base_url': 'https://domo-support.domo.com',\n",
       " 'scrape_crawler': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task = test_scrape_tasks[2][\"config\"]\n",
    "test_task.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US to ./SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK',\n",
       " 'https://domo-support.domo.com/s/knowledge-base',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOnGAK',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_generator = dg.DriverGenerator()\n",
    "driver = driver_generator.get_webdriver()\n",
    "\n",
    "test_task.scrape_page(driver=driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Crawler\n",
    "\n",
    "The Scrape_Crawler is generally just a crawler that receieves a URL then manages scraping that page (by retrieving the correct scrape_configuration process) and searching for URLs to extract from that page and adding it to the `urls_to_visit` property.\n",
    "\n",
    "The ThreadPoolExecutor has a maximum number of threads, maximum_workers, it will use.\n",
    "\n",
    "This crawler does not concern itself with the return of the webcrawl task, because the task handles downloading the HTML file to a parameterized location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Crawler:\n",
    "    \"\"\"threadpool manager for crawling through a list of urls\"\"\"\n",
    "\n",
    "    executor: ThreadPoolExecutor\n",
    "    scrape_factory: Scrape_Factory\n",
    "    base_url: str\n",
    "\n",
    "    download_folder: str\n",
    "    visited_urls: set\n",
    "    urls_to_visit: set\n",
    "\n",
    "    driver_generator: dg.DriverGenerator = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        driver_path,\n",
    "        scrape_factory: Scrape_Factory,\n",
    "        urls_to_visit: list,\n",
    "        base_url: str,\n",
    "        urls_visited: list = None,\n",
    "        max_workers=5,\n",
    "        download_folder: str = \"./SCRAPE/\",\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.scrape_factory = scrape_factory\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.driver_generator = dg.DriverGenerator(driver_path=driver_path)\n",
    "        self.download_folder = download_folder\n",
    "\n",
    "        self.visited_urls = set()\n",
    "        if urls_visited:\n",
    "            [self._add_url_to_visited(url) for url in urls_visited]\n",
    "\n",
    "        self.urls_to_visit = set()\n",
    "\n",
    "        if urls_to_visit:\n",
    "            [self._add_url_to_visit(url) for url in urls_to_visit]\n",
    "\n",
    "    def _add_url_to_visit(self, url, debug_prn: bool = False):\n",
    "        \"\"\"adds a URL to the list of URLS to visit after testing that the URL has not already been visited\"\"\"\n",
    "\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            if debug_prn:\n",
    "                print(f\"adding {url} to to_vist list\")\n",
    "\n",
    "            self.urls_to_visit.add(url)\n",
    "            return self.urls_to_visit\n",
    "\n",
    "    def _add_url_to_visited(self, url, debug_prn: bool = False):\n",
    "        if url not in self.visited_urls:\n",
    "            if debug_prn:\n",
    "                print(f\"adding {url} to visited list\")\n",
    "\n",
    "            self.visited_urls.add(url)\n",
    "            return self.visited_urls\n",
    "\n",
    "    def _quit(self):\n",
    "        \"\"\"call when the executor queue is empty\"\"\"\n",
    "\n",
    "        self.executor.shutdown(wait=True)\n",
    "        return f\"Done scraping {len(self.visited_urls)} urls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_url': 'https://domo-support.domo.com',\n",
       " 'scrape_factory': Scrape_Factory(factory_configs=[Scrape_Config(pattern='.*/s/article/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f793342b6a0>, generate_filename_fn=<function generate_filename_from_url at 0x7f7932e714e0>, content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f7932e71da0>, search_element_type='class name', search_element_text='slds-form-element', max_sleep_time=10), Scrape_Config(pattern='.*/s/topic/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f793342b6a0>, generate_filename_fn=<function generate_filename_from_url at 0x7f7932e714e0>, content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f79b67dc5e0>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item', max_sleep_time=10), Scrape_Config(pattern='.*/s/knowledge-base.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f793342b6a0>, generate_filename_fn=<function generate_filename_from_url at 0x7f7932e714e0>, content_extractor_fn=<function domokb_knowledgebase_content_extractor_fn at 0x7f7932e70c20>, search_element_type='css selector', search_element_text='.topic-nav-container, .cDomoKBCategoryNav', max_sleep_time=10)]),\n",
       " 'executor': <concurrent.futures.thread.ThreadPoolExecutor at 0x7f79333ffa50>,\n",
       " 'driver_generator': <gdoc_sync.scraper.driver.DriverGenerator at 0x7f7932e99a50>,\n",
       " 'download_folder': './SCRAPE/',\n",
       " 'visited_urls': set(),\n",
       " 'urls_to_visit': {'https://domo-support.domo.com/s/article/36004740075?language=en_US',\n",
       "  'https://domo-support.domo.com/s/knowledge-base?language=en_US',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    urls_to_visit=test_urls,\n",
    ")\n",
    "wbs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Crawler)\n",
    "def crawl_urls(self: Scrape_Crawler, is_test: bool = False, debug_prn: bool = False):\n",
    "    while self.urls_to_visit:\n",
    "        url = self.urls_to_visit.pop()\n",
    "        self.visited_urls.add(url)\n",
    "\n",
    "        driver = self.driver_generator.get_webdriver()\n",
    "\n",
    "        config = self.scrape_factory.get_factory_config(\n",
    "            url, debug_prn=debug_prn, scrape_crawler=self\n",
    "        )\n",
    "\n",
    "        future = self.executor.submit(\n",
    "            config.scrape_page, driver=driver, is_test=is_test\n",
    "        )\n",
    "        try:\n",
    "            urls_to_visit, visited_urls = future.result()\n",
    "\n",
    "            print(urls_to_visit, visited_urls)\n",
    "\n",
    "            if urls_to_visit:\n",
    "                [self._add_url_to_visit(url) for url in urls_to_visit]\n",
    "\n",
    "            if visited_urls:\n",
    "                [self._add_url_to_visited(url) for url in visited_urls]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return self._quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Scrape_Factory.get_factory_config() missing 1 required positional argument: 'base_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m wbs \u001b[38;5;241m=\u001b[39m Scrape_Crawler(\n\u001b[1;32m      2\u001b[0m     driver_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr//bin/chromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     scrape_factory\u001b[38;5;241m=\u001b[39mDomoKB_ScrapeFactory,\n\u001b[1;32m      4\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://domo-support.domo.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     urls_to_visit \u001b[38;5;241m=\u001b[39m test_urls\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mwbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m wbs\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "Cell \u001b[0;32mIn[52], line 16\u001b[0m, in \u001b[0;36mcrawl_urls\u001b[0;34m(self, is_test, debug_prn)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisited_urls\u001b[38;5;241m.\u001b[39madd(url)\n\u001b[1;32m     14\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_generator\u001b[38;5;241m.\u001b[39mget_webdriver()\n\u001b[0;32m---> 16\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_factory_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_prn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_prn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscrape_crawler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39msubmit(config\u001b[38;5;241m.\u001b[39mscrape_page, driver \u001b[38;5;241m=\u001b[39m driver, is_test \u001b[38;5;241m=\u001b[39m is_test)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Scrape_Factory.get_factory_config() missing 1 required positional argument: 'base_url'"
     ]
    }
   ],
   "source": [
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    urls_to_visit=test_urls,\n",
    ")\n",
    "\n",
    "wbs.crawl_urls()\n",
    "\n",
    "wbs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# import nbdev\n",
    "\n",
    "# nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
