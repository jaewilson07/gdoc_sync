{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: scraper.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp scraper.Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "import re\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Callable, Any\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from gdoc_sync.utils import upsert_folder, convert_str_file_name\n",
    "\n",
    "import gdoc_sync.scraper.driver as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "from nbdev.showdoc import patch_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromeDriver 120.0.6099.109 (3419140ab665596f21b385ce136419fde0924272-refs/branch-heads/6099@{#1483})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from selenium.webdriver.common.by import By\n",
    "\n",
    "test_urls = [\n",
    "    \"https://domo-support.domo.com/s/article/36004740075?language=en_US\",\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US\",  # list of articles\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US\",  # list of topics\n",
    "]\n",
    "\n",
    "drivergenerator = dg.DriverGenerator(debug_prn=True)\n",
    "\n",
    "driver = drivergenerator.get_webdriver()\n",
    "\n",
    "test_soup = dg.get_pagesource(\n",
    "    driver=driver,\n",
    "    url=\"https://domo-support.domo.com/s/article/36004740075?language=en_US\",\n",
    "    search_criteria_tuple=(By.CLASS_NAME, \"slds-form-element\"),\n",
    "    max_sleep_time=15,\n",
    "    return_soup=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_links(\n",
    "    soup: BeautifulSoup, base_url: str = None, custom_link_extractor_fn: Callable = None\n",
    ") -> [str]:\n",
    "    \"\"\"returns a list of urls\"\"\"\n",
    "\n",
    "    links_ls = [\n",
    "        link[\"href\"]\n",
    "        for link in soup.findAll(\"a\")\n",
    "        if (not base_url and link.has_attr(\"href\"))\n",
    "        or (\n",
    "            base_url\n",
    "            and link.has_attr(\"href\")\n",
    "            and link[\"href\"].lower().startswith(base_url.lower())\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if custom_link_extractor_fn:\n",
    "        return custom_link_extractor_fn(links_ls, base_url)\n",
    "\n",
    "    return links_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36004740075?nocache=https%3A%2F%2Fdomo-support.domo.com%2Fs%2Farticle%2F36004740075%3Flanguage%3Den_US',\n",
       " 'https://www.domo.com/domo-central',\n",
       " 'https://www.domo.com/domo-central/community',\n",
       " 'https://community-forums.domo.com/main',\n",
       " 'https://community-forums.domo.com/main/categories/welcome']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_links(test_soup)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def generate_filename_from_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    return \"_\".join([str for str in parsed_url[2].split(\"/\") if str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s_article_36004740075',\n",
       " 's_topic_0TO5w000000ZlOmGAK_20202023',\n",
       " 's_topic_0TO5w000000Zan7GAC_archived-feature-release-notes']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[generate_filename_from_url(url) for url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Config:\n",
    "    \"\"\"class for collating data about how to scrape a page\"\"\"\n",
    "\n",
    "    pattern: re.Pattern  # url pattern\n",
    "    link_extractor_fn: Callable = extract_links\n",
    "\n",
    "    generate_filename_fn: Callable = generate_filename_from_url\n",
    "    content_extractor_fn: Callable = None\n",
    "\n",
    "    search_element_type: Any = None\n",
    "    search_element_text: str = None\n",
    "\n",
    "    max_sleep_time: int = 10\n",
    "\n",
    "    def __id__(self, other):\n",
    "        return self.pattern == other.pattern\n",
    "\n",
    "    def get_search_tuple(self) -> Any:\n",
    "        if not self.search_element_text and self.search_element_type:\n",
    "            return None\n",
    "\n",
    "        return (self.search_element_type, self.search_element_text)\n",
    "\n",
    "    def is_text_match_pattern(self, text, debug_prn: bool = False):\n",
    "        pattern = re.compile(self.pattern)\n",
    "\n",
    "        match_pattern = pattern.match(text)\n",
    "\n",
    "        if debug_prn:\n",
    "            print({\"text\": text, \"pattern\": self.pattern})\n",
    "\n",
    "        if not match_pattern:\n",
    "            return False\n",
    "\n",
    "        if debug_prn:\n",
    "            print(match_pattern)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScrapeConfig manages content download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Scrape_Config)\n",
    "def _download_content(self: Scrape_Config, folder_path, file_name, content):\n",
    "    upsert_folder(folder_path)\n",
    "\n",
    "    with open(os.path.join(folder_path, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(content))\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Config)\n",
    "def scrape_page(self: Scrape_Config, url, debug_prn: bool = True):\n",
    "    if debug_prn:\n",
    "        print(f\"scraping_page {url}\")\n",
    "\n",
    "    soup = None\n",
    "    try:\n",
    "        soup = self.get_pagesource(url=url)\n",
    "\n",
    "        scrape_config = self.scrape_factory.get_factory_config(url)\n",
    "\n",
    "        upsert_folder(self.download_folder)\n",
    "\n",
    "        folder_path = (\n",
    "            os.path.join(\n",
    "                self.download_folder, scrape_config.convert_url_to_filename(url)\n",
    "            )\n",
    "            + \"/\"\n",
    "        )\n",
    "\n",
    "        self._download_content(\n",
    "            folder_path=folder_path, file_name=\"index.html\", content=soup\n",
    "        )\n",
    "\n",
    "        content_scraper = scrape_config.content_scraper\n",
    "\n",
    "        # Extract the article content\n",
    "        if content_scraper:\n",
    "            content = content_scraper(soup)\n",
    "\n",
    "            self._download_content(\n",
    "                folder_path=folder_path, file_name=\"content.html\", content=content\n",
    "            )\n",
    "        if self.is_test:\n",
    "            print(\"this is a test\")\n",
    "\n",
    "        if not self.is_test:\n",
    "            links = get_links(soup, self.base_url)\n",
    "            [self._add_url_to_visit(link) for link in links]\n",
    "        return f\"ðŸŽ‰ successfully scraped {url}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"ðŸ’€ failed to download {url} received errror{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Scrape_Factory_NoConfigMatch(Exception):\n",
    "    def __init__(self, text):\n",
    "        super().__init__(\n",
    "            f\"{text} has no pattern match in factory_configs, add an appropriate config or check pattern matches\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Factory:\n",
    "    \"\"\"class handles a list of Scrape_Configs and will return the 'correct one' given a URL\"\"\"\n",
    "\n",
    "    factory_configs: List[Scrape_Config]\n",
    "\n",
    "    def get_factory_config(self, url, debug_prn: bool = False):\n",
    "        config = next(\n",
    "            (\n",
    "                config\n",
    "                for config in self.factory_configs\n",
    "                if config.is_text_match_pattern(url, debug_prn=debug_prn)\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if not config:\n",
    "            raise Scrape_Factory_NoConfigMatch(text=url)\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DomoKB_ScrapeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def process_link(link, base_url):\n",
    "    url = link.get(\"href\")\n",
    "\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    # for relative addresses concat base_url\n",
    "    if url.startswith(\"/s/\"):\n",
    "        url = urljoin(base_url, url)\n",
    "\n",
    "    # ignore urls not orginating from base_url\n",
    "    if not url.startswith(base_url):\n",
    "        return None\n",
    "\n",
    "    # remove query params\n",
    "    url = urljoin(url, urlparse(url).path)\n",
    "\n",
    "    # only keep the first 6 pieces of the URL\n",
    "    url = \"/\".join(url.split(\"/\")[:6])\n",
    "\n",
    "    if url.endswith(\"/\"):\n",
    "        url = url[:-1]\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def domokb_link_extractor_fn(soup, base_url):\n",
    "    links = []\n",
    "\n",
    "    for link in soup.findAll(\"a\"):\n",
    "        url = process_link(link, base_url)\n",
    "\n",
    "        if url and url not in links:\n",
    "            links.append(url)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "def domokb_article_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"article-column\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Article = Scrape_Config(\n",
    "    pattern=r\".*/s/article/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_article_content_extractor_fn,\n",
    "    search_element_type=By.CLASS_NAME,\n",
    "    search_element_text=\"slds-form-element\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def domokb_topic_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"knowledge-base\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Topic = Scrape_Config(\n",
    "    pattern=r\".*/s/topic/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_topic_content_extractor_fn,\n",
    "    search_element_type=By.CSS_SELECTOR,\n",
    "    search_element_text=f\".{', .'.join(['section-list-item', 'article-list-item'] )}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "DomoKB_ScrapeFactory = Scrape_Factory(\n",
    "    [DomoKB_ScrapeConfig_Article, DomoKB_ScrapeConfig_Topic]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://domo-support.domo.com/s/article/36004740075?language=en_US',\n",
       "  'config': Scrape_Config(pattern='.*/s/article/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f97038200e0>, generate_filename_fn=<function generate_filename_from_url at 0x7f970411d580>, content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f970411ff60>, search_element_type='class name', search_element_text='slds-form-element', max_sleep_time=10)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US',\n",
       "  'config': Scrape_Config(pattern='.*/s/topic/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f97038200e0>, generate_filename_fn=<function generate_filename_from_url at 0x7f970411d580>, content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f9703820220>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item', max_sleep_time=10)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US',\n",
       "  'config': Scrape_Config(pattern='.*/s/topic/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f97038200e0>, generate_filename_fn=<function generate_filename_from_url at 0x7f970411d580>, content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f9703820220>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item', max_sleep_time=10)}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    {\n",
    "        \"url\": url,\n",
    "        \"config\": DomoKB_ScrapeFactory.get_factory_config(url, debug_prn=False),\n",
    "    }\n",
    "    for url in test_urls\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Scrape_Crawler:\n",
    "    \"\"\"threadpool manager for crawling through a list of urls\"\"\"\n",
    "\n",
    "    executor: ThreadPoolExecutor\n",
    "    scrape_factory: Scrape_Factory\n",
    "    base_url: str\n",
    "\n",
    "    driver_generator: dg.DriverGenerator = None\n",
    "\n",
    "    download_folder: str\n",
    "\n",
    "    is_test: bool\n",
    "    visited_urls: set = set()\n",
    "    urls_to_visit: set = set()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        driver_path,\n",
    "        scrape_factory: Scrape_Factory,\n",
    "        base_url: str,\n",
    "        is_test: bool = False,\n",
    "        max_workers=5,\n",
    "        download_folder: str = \"./SCRAPE/\",\n",
    "    ):\n",
    "        self.is_test = is_test\n",
    "        self.base_url = base_url\n",
    "        self.scrape_factory = scrape_factory\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.driver_generator = dg.DriverGenerator(driver_path=driver_path)\n",
    "        self.download_folder = download_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_test': False,\n",
       " 'base_url': 'https://domo-support.domo.com',\n",
       " 'scrape_factory': Scrape_Factory(factory_configs=[Scrape_Config(pattern='.*/s/article/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f97038200e0>, generate_filename_fn=<function generate_filename_from_url at 0x7f970411d580>, content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f970411ff60>, search_element_type='class name', search_element_text='slds-form-element', max_sleep_time=10), Scrape_Config(pattern='.*/s/topic/.*', link_extractor_fn=<function domokb_link_extractor_fn at 0x7f97038200e0>, generate_filename_fn=<function generate_filename_from_url at 0x7f970411d580>, content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f9703820220>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item', max_sleep_time=10)]),\n",
       " 'executor': <concurrent.futures.thread.ThreadPoolExecutor at 0x7f970380d950>,\n",
       " 'driver_generator': <gdoc_sync.scraper.driver.DriverGenerator at 0x7f97039e5310>,\n",
       " 'download_folder': './SCRAPE/'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    ")\n",
    "wbs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Crawler)\n",
    "def get_pagesource(self: Scrape_Crawler, url: str, debug_prn: bool = False):\n",
    "    # Find the first pattern that matches the URL and get the corresponding attribute\n",
    "\n",
    "    scrape_config = self.scrape_factory.get_factory_config(url)\n",
    "\n",
    "    assert scrape_config\n",
    "\n",
    "    driver = self.driver_generator.get_webdriver()\n",
    "\n",
    "    search_criteria_tuple = scrape_config.get_search_tuple()\n",
    "    max_sleep_time = scrape_config.max_sleep_time\n",
    "\n",
    "    if debug_prn:\n",
    "        print(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"max_sleep_time\": max_sleep_time,\n",
    "                \"search_criteria\": search_criteria_tuple,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    pagesource = dg.get_pagesource(\n",
    "        url=url,\n",
    "        search_criteria_tuple=search_criteria_tuple,\n",
    "        driver=driver,\n",
    "        max_sleep_time=max_sleep_time,\n",
    "    )\n",
    "\n",
    "    if not pagesource:\n",
    "        raise Exception(f\"unable to retrieve source {url}\")\n",
    "\n",
    "    return pagesource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html dir=\"ltr\" lang=\"en-US\"><head><title>Transforming Data In Domo</title><meta content=\"default-sr'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url = \"https://domo-support.domo.com/s/article/36004740075?language=en_US\"\n",
    "test_url = \"https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC/transforming-data-in-domo?language=en_US\"\n",
    "\n",
    "# generate a chrome webdriver for scraping the webpage\n",
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    ")\n",
    "\n",
    "soup = wbs.get_pagesource(url=test_url)\n",
    "\n",
    "str(soup)[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape_Crawler manages URls visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WebScraper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# | export\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;129m@patch_to\u001b[39m(\u001b[43mWebScraper\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_url_to_visit\u001b[39m(\u001b[38;5;28mself\u001b[39m: WebScraper, url):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisited_urls:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to to_vist list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WebScraper' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Crawler)\n",
    "def _add_url_to_visit(self: Scrape_Crawler, url):\n",
    "    if url not in self.visited_urls:\n",
    "        print(f\"adding {url} to to_vist list\")\n",
    "\n",
    "        self.urls_to_visit.add(url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://domo-support.domo.com/s/article/36004740075?language=en_US\"\n",
    "test_url = \"https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC/transforming-data-in-domo?language=en_US\"\n",
    "\n",
    "# generate a chrome webdriver for scraping the webpage\n",
    "wbs = WebScraper(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=Scrape_Factory(factory_configs),\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    ")\n",
    "\n",
    "wbs.scrape_page(url=test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch_to(WebScraper)\n",
    "def quit(self: WebScraper):\n",
    "    self.executor.shutdown(wait=True)\n",
    "    return f\"Done scraping {len(self.visited_urls)} urls\"\n",
    "\n",
    "\n",
    "@patch_to(WebScraper)\n",
    "def scrape_next_page(self: WebScraper):\n",
    "    while self.urls_to_visit:\n",
    "        url = self.urls_to_visit.pop()\n",
    "        self.visited_urls.add(url)\n",
    "        future = self.executor.submit(self.scrape_page, url)\n",
    "        try:\n",
    "            result = future.result()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return self.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://domo-support.domo.com/s/article/36004740075?language=en_US\"\n",
    "test_url = \"https://domo-support.domo.com/s/topic/0TO5w000000ZanUGAS/dataflow-management?language=en_US\"\n",
    "test_url = \"https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC/transforming-data-in-domo?language=en_US\"\n",
    "\n",
    "wbs = WebScraper(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=Scrape_Factory(factory_configs),\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    ")\n",
    "\n",
    "wbs._add_url_to_visit(\n",
    "    test_url,\n",
    ")\n",
    "\n",
    "wbs.scrape_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# import nbdev\n",
    "\n",
    "# nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
