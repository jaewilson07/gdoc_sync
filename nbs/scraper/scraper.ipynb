{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: scraper.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp scraper.Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "import re\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Callable, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from __future__ import annotations\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from gdoc_sync.utils import upsert_folder, convert_str_file_name\n",
    "\n",
    "import gdoc_sync.scraper.driver as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "from nbdev.showdoc import patch_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from selenium.webdriver.common.by import By\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromeDriver 120.0.6099.109 (3419140ab665596f21b385ce136419fde0924272-refs/branch-heads/6099@{#1483})\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<html dir=\"ltr\" lang=\"en-US\"><head><title>Knowledge Base</title><meta content=\"default-src \\'self\\'; s'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from selenium.webdriver.common.by import By\n",
    "\n",
    "test_urls = [\n",
    "    \"https://domo-support.domo.com/s/article/36004740075?language=en_US\",\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US\",  # list of articles\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US\",  # list of topics\n",
    "    \"https://domo-support.domo.com/s/knowledge-base?language=en_US\",\n",
    "]\n",
    "\n",
    "drivergenerator = dg.DriverGenerator(debug_prn=True)\n",
    "\n",
    "driver = drivergenerator.get_webdriver()\n",
    "\n",
    "test_soup = dg.get_pagesource(\n",
    "    driver=driver,\n",
    "    url=test_urls[3],\n",
    "    search_criteria_tuple=(By.CLASS_NAME, \"topic-nav-container\"),\n",
    "    max_sleep_time=15,\n",
    "    return_soup=True,\n",
    ")\n",
    "\n",
    "# test_soup = dg.get_pagesource(\n",
    "#     driver=driver,\n",
    "#     url=test_urls[2],\n",
    "#     search_criteria_tuple=(\n",
    "#         By.CSS_SELECTOR,\n",
    "#         f\".{', .'.join(['section-list-item', 'article-list-item'] )}\",\n",
    "#     ),\n",
    "#     max_sleep_time=15,\n",
    "#     return_soup=True,\n",
    "# )\n",
    "\n",
    "str(test_soup)[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils for Processing Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_query_params_from_url(url):\n",
    "    u = urlparse(url)\n",
    "    return urljoin(url, urlparse(url).path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/article/36004740075',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes',\n",
       " 'https://domo-support.domo.com/s/knowledge-base']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[remove_query_params_from_url(url) for url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_links(\n",
    "    soup: BeautifulSoup,\n",
    "    base_url: str = None,\n",
    "    custom_link_extractor_fn: Callable = None,\n",
    "    debug_prn: bool = False,\n",
    ") -> [str]:\n",
    "    \"\"\"returns a list of urls\"\"\"\n",
    "\n",
    "    links_ls = []\n",
    "\n",
    "    for link in soup.findAll(\"a\"):\n",
    "        if not link.has_attr(\"href\"):\n",
    "            continue\n",
    "\n",
    "        url = link[\"href\"]\n",
    "\n",
    "        if debug_prn:\n",
    "            print(url)\n",
    "\n",
    "        if url.startswith(\"/\") and base_url:\n",
    "            url = urljoin(base_url, url)\n",
    "\n",
    "        if base_url and not url.startswith(base_url):\n",
    "            continue\n",
    "\n",
    "        if custom_link_extractor_fn:\n",
    "            url = custom_link_extractor_fn(url, base_url)\n",
    "\n",
    "        if url in links_ls:\n",
    "            continue\n",
    "\n",
    "        links_ls.append(url)\n",
    "\n",
    "    return list(set(links_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC/release-notes?language=en_US',\n",
       " 'https://domo-support.domo.com/s/knowledge-base?language=en_US']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_links(\n",
    "    test_soup,  # result of dg.get_pagesource()\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    debug_prn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def generate_filename_from_url(url, base_folder=None, file_name=None) -> str:\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    base_str = \"_\".join([str for str in parsed_url[2].split(\"/\") if str])\n",
    "\n",
    "    if base_folder:\n",
    "        base_str = os.path.join(base_folder, base_str)\n",
    "\n",
    "    if file_name:\n",
    "        base_str = os.path.join(base_str, file_name)\n",
    "\n",
    "    return base_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s_article_36004740075',\n",
      " 's_topic_0TO5w000000ZlOmGAK_20202023',\n",
      " 's_topic_0TO5w000000Zan7GAC_archived-feature-release-notes',\n",
      " 's_knowledge-base']\n",
      "-- alternative with base_url and different file name\n",
      "['/SCRAPE/s_article_36004740075/index.html',\n",
      " '/SCRAPE/s_topic_0TO5w000000ZlOmGAK_20202023/index.html',\n",
      " '/SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html',\n",
      " '/SCRAPE/s_knowledge-base/index.html']\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "pprint([generate_filename_from_url(url) for url in test_urls])\n",
    "\n",
    "print(\"-- alternative with base_url and different file name\")\n",
    "pprint([generate_filename_from_url(url, \"/SCRAPE\", \"index.html\") for url in test_urls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Config:\n",
    "    \"\"\"class for collating data about how to scrape a page\"\"\"\n",
    "\n",
    "    pattern: re.Pattern  # url pattern\n",
    "    content_extractor_fn: Callable = None\n",
    "    link_extractor_fn: Callable = extract_links\n",
    "\n",
    "    search_element_type: Any = None\n",
    "    search_element_text: str = None\n",
    "\n",
    "    def __id__(self, other):\n",
    "        return self.pattern == other.pattern\n",
    "\n",
    "    def get_search_tuple(self) -> Any:\n",
    "        if not self.search_element_text and self.search_element_type:\n",
    "            return None\n",
    "\n",
    "        return (self.search_element_type, self.search_element_text)\n",
    "\n",
    "    def is_text_match_pattern(self, text, debug_prn: bool = False):\n",
    "        pattern = re.compile(self.pattern)\n",
    "\n",
    "        match_pattern = pattern.match(text)\n",
    "\n",
    "        if debug_prn:\n",
    "            print({\"text\": text, \"pattern\": self.pattern})\n",
    "\n",
    "        if not match_pattern:\n",
    "            return False\n",
    "\n",
    "        if debug_prn:\n",
    "            print(match_pattern)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ScrapeTask_NoDriverProvided(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Driver not provided\")\n",
    "\n",
    "\n",
    "class ScrapeTask_NoBaseUrl(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            \"no base_url provided to scrape_task. must run as is_test = True\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ScrapeTask_NoScrapeCrawler(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            \"no scrape_crawler provided.  must run scrape_page as is_test = True\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Task:\n",
    "    url: str  # will be assigned after retrieved from scrape_factory\n",
    "\n",
    "    base_url: str = None\n",
    "    download_folder: str = None\n",
    "\n",
    "    generate_filename_fn: Callable = generate_filename_from_url\n",
    "\n",
    "    max_sleep_time: int = 10\n",
    "    scrape_config: Scrape_Config = None  # to define custom ways of extracting content\n",
    "    scrape_crawler: Scrape_Crawler = None  # will be optional parent threadpool manager.\n",
    "\n",
    "    @classmethod\n",
    "    def _from_factory(\n",
    "        cls,\n",
    "        url,\n",
    "        base_url,\n",
    "        scrape_config,\n",
    "        scrape_crawler=None,\n",
    "        download_folder=None,\n",
    "    ):\n",
    "        return cls(\n",
    "            url=url,\n",
    "            base_url=base_url,\n",
    "            download_folder=download_folder,\n",
    "            scrape_config=scrape_config,\n",
    "            scrape_crawler=scrape_crawler,\n",
    "        )\n",
    "\n",
    "    def _get_pagesource(self, driver=None, debug_prn: bool = False):\n",
    "        \"\"\"\n",
    "        handles fetching the pagesource, html content of a URL\n",
    "        \"\"\"\n",
    "\n",
    "        url = self.url\n",
    "\n",
    "        driver = (\n",
    "            driver\n",
    "            or (\n",
    "                self.scrape_crawler\n",
    "                and self.scrape_crawler.driver_generator.get_driver()\n",
    "            )\n",
    "            or None\n",
    "        )\n",
    "\n",
    "        if not driver:\n",
    "            raise ScrapeTask_NoDriverProvided()\n",
    "\n",
    "        search_criteria_tuple = (\n",
    "            self.scrape_config and self.scrape_config.get_search_tuple()\n",
    "        ) or None\n",
    "\n",
    "        max_sleep_time = self.max_sleep_time or 15\n",
    "\n",
    "        if debug_prn:\n",
    "            print(\n",
    "                {\n",
    "                    \"url\": url,\n",
    "                    \"max_sleep_time\": max_sleep_time,\n",
    "                    \"search_criteria\": search_criteria_tuple,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return dg.get_pagesource(\n",
    "            url=url,\n",
    "            search_criteria_tuple=search_criteria_tuple,\n",
    "            driver=driver,\n",
    "            max_sleep_time=max_sleep_time,\n",
    "        )\n",
    "\n",
    "    def _download_content(self, file_name, content):\n",
    "        dir_name = os.path.dirname(file_name)\n",
    "\n",
    "        if dir_name[-1] != \"/\":\n",
    "            dir_name += \"/\"\n",
    "\n",
    "        upsert_folder(dir_name)\n",
    "\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(content))\n",
    "\n",
    "        return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Task)\n",
    "def scrape_page(\n",
    "    self: Scrape_Task,\n",
    "    base_url=None,\n",
    "    driver=None,\n",
    "    download_folder=None,\n",
    "    debug_prn: bool = False,\n",
    "    is_suppress_errors: bool = False,\n",
    "    is_test: bool = False,\n",
    "):\n",
    "    url = self.url\n",
    "\n",
    "    download_folder = download_folder or self.download_folder or \"./SCRAPE\"\n",
    "\n",
    "    scrape_crawler = self.scrape_crawler\n",
    "\n",
    "    driver = (\n",
    "        driver\n",
    "        or (scrape_crawler and scrape_crawler.driver_generator.get_webdriver())\n",
    "        or None\n",
    "    )\n",
    "\n",
    "    if not driver:\n",
    "        raise ScrapeTask_NoDriverProvided()\n",
    "\n",
    "    base_url = (\n",
    "        base_url\n",
    "        or self.base_url\n",
    "        or (scrape_crawler and scrape_crawler.base_url)\n",
    "        or None\n",
    "    )\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"scraping_page {url}\")\n",
    "\n",
    "    try:\n",
    "        soup = self._get_pagesource(driver=driver, debug_prn=debug_prn)\n",
    "\n",
    "        file_name = self.generate_filename_fn(\n",
    "            url=url, base_folder=download_folder, file_name=\"index.html\"\n",
    "        )\n",
    "\n",
    "        save_location = self._download_content(file_name=file_name, content=soup)\n",
    "\n",
    "        # Extract the article content\n",
    "        if self.scrape_config:\n",
    "            content = self.scrape_config.content_extractor_fn(soup)\n",
    "\n",
    "            content_name = self.generate_filename_fn(\n",
    "                url=url, base_folder=download_folder, file_name=\"content.html\"\n",
    "            )\n",
    "\n",
    "            self._download_content(\n",
    "                content_name,\n",
    "                content=content,\n",
    "            )\n",
    "\n",
    "        print(f\"🎉 successfully scraped {url} to {save_location}\")\n",
    "\n",
    "        if is_test:\n",
    "            return []\n",
    "\n",
    "        if not base_url:\n",
    "            raise ScrapeTask_NoBaseUrl()\n",
    "\n",
    "        if not scrape_crawler:\n",
    "            raise ScrapeTask_NoScrapeCrawler()\n",
    "\n",
    "        \"\"\"if config is part of a scrape_crawler / managed threadpool will update crawler's urls_to_visit list.\"\"\"\n",
    "\n",
    "        urls_to_visit = self.scrape_config.link_extractor_fn(soup, base_url)\n",
    "\n",
    "        scrape_crawler._add_url_to_visited(url)\n",
    "\n",
    "        for url in urls_to_visit:\n",
    "            scrape_crawler._add_url_to_visit(url)\n",
    "\n",
    "        return urls_to_visit\n",
    "\n",
    "    except Exception as e:\n",
    "        if not is_suppress_errors:\n",
    "            raise (e)\n",
    "        return f\"💀 failed to download {url} received errror{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html dir=\"ltr\" lang=\"en-US\"><head><title>Article Detail</title><meta content=\"default-src \\'self\\'; s'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Scrape_Task(\n",
    "    url=test_urls[0],\n",
    ")\n",
    "\n",
    "str(task._get_pagesource(driver=driver))[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/article/36004740075?language=en_US to ./SCRAPE/s_article_36004740075/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Scrape_Task(\n",
    "    url=test_urls[0],\n",
    ")\n",
    "\n",
    "task.scrape_page(driver=driver, base_url=\"https://domo-support.domo.com\", is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Factory converts URL into Scrape_Config task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Scrape_Factory_NoConfigMatch(Exception):\n",
    "    def __init__(self, text):\n",
    "        super().__init__(\n",
    "            f\"{text} has no pattern match in factory_configs, add an appropriate config or check pattern matches\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Factory:\n",
    "    \"\"\"class handles a list of Scrape_Configs and will return the 'correct one' given a URL\"\"\"\n",
    "\n",
    "    factory_configs: List[Scrape_Config]\n",
    "\n",
    "    def get_task(\n",
    "        self,\n",
    "        url,\n",
    "        download_folder=\"./SCRAPE\",\n",
    "        base_url=None,\n",
    "        scrape_crawler=None,\n",
    "        debug_prn: bool = False,\n",
    "    ):\n",
    "        config = next(\n",
    "            (\n",
    "                config\n",
    "                for config in self.factory_configs\n",
    "                if config.is_text_match_pattern(url, debug_prn=debug_prn)\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if not config:\n",
    "            raise Scrape_Factory_NoConfigMatch(text=url)\n",
    "\n",
    "        return Scrape_Task._from_factory(\n",
    "            scrape_config=config,\n",
    "            url=url,\n",
    "            base_url=base_url,\n",
    "            download_folder=download_folder,\n",
    "            scrape_crawler=scrape_crawler,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeFactory\n",
    "\n",
    "An instance of Scrape Factory for scraping Domo KBs three types of pages, Articles, Topic lists, and the main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def process_domo_kb_link(url, base_url):\n",
    "    # remove query params\n",
    "    url = remove_query_params_from_url(url)\n",
    "\n",
    "    # only keep the first 6 pieces of the URL\n",
    "    url = \"/\".join(url.split(\"/\")[:6])\n",
    "\n",
    "    if url.endswith(\"/\"):\n",
    "        url = url[:-1]\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def domokb_link_extractor_fn(soup, base_url, debug_prn: bool = False):\n",
    "    \"\"\"custom link extractor for processing Domo KBs.\n",
    "    will be embedded into all DomoKB_ScrapeConfigs\n",
    "    \"\"\"\n",
    "    return extract_links(\n",
    "        soup,\n",
    "        custom_link_extractor_fn=process_domo_kb_link,\n",
    "        base_url=base_url,\n",
    "        debug_prn=debug_prn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/knowledge-base',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "domokb_link_extractor_fn(test_soup, \"https://domo-support.domo.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def domokb_article_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"article-column\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Article = Scrape_Config(\n",
    "    pattern=r\".*/s/article/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_article_content_extractor_fn,\n",
    "    search_element_type=By.CLASS_NAME,\n",
    "    search_element_text=\"slds-form-element\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def domokb_topic_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"knowledge-base\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Topic = Scrape_Config(\n",
    "    pattern=r\".*/s/topic/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_topic_content_extractor_fn,\n",
    "    search_element_type=By.CSS_SELECTOR,\n",
    "    search_element_text=f\".{', .'.join(['section-list-item', 'article-list-item'] )}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_NavContainer\n",
    "cDomoKBCategoryNav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def domokb_knowledgebase_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"knowledge-base\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_KnowledgeBase = Scrape_Config(\n",
    "    pattern=r\".*/s/knowledge-base.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_knowledgebase_content_extractor_fn,\n",
    "    search_element_type=By.CSS_SELECTOR,\n",
    "    search_element_text=f\".{', .'.join(['topic-nav-container', 'cDomoKBCategoryNav'] )}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeFactory\n",
    "A factory pattern receives a parameter (the url to match to) then returns the appropriate implementation `Scrape_Config` for handling that url.\n",
    "\n",
    "This pattern could be extended for any type of website; however we have implemented a factory specifically for handling Domo Kbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "DomoKB_ScrapeFactory = Scrape_Factory(\n",
    "    [\n",
    "        DomoKB_ScrapeConfig_Article,\n",
    "        DomoKB_ScrapeConfig_Topic,\n",
    "        DomoKB_ScrapeConfig_KnowledgeBase,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation\n",
    "\n",
    "Of retrieving the scrape_config using the `scrape_factory.get_factory_config() method\n",
    "\n",
    "- test_urls is a list of URLS\n",
    "- use a list comprenension to show we can retrieve the correct config for different URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://domo-support.domo.com/s/article/36004740075?language=en_US',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/article/36004740075?language=en_US', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f8d606ffa60>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/article/.*', content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f8ce44ec2c0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='class name', search_element_text='slds-form-element'), scrape_crawler=None)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f8d606ffa60>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f8ce44ee2a0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'), scrape_crawler=None)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f8d606ffa60>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f8ce44ee2a0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'), scrape_crawler=None)},\n",
       " {'url': 'https://domo-support.domo.com/s/knowledge-base?language=en_US',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/knowledge-base?language=en_US', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f8d606ffa60>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/knowledge-base.*', content_extractor_fn=<function domokb_knowledgebase_content_extractor_fn at 0x7f8ce44eda80>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='css selector', search_element_text='.topic-nav-container, .cDomoKBCategoryNav'), scrape_crawler=None)}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scrape_tasks = [\n",
    "    {\n",
    "        \"url\": url,\n",
    "        \"config\": DomoKB_ScrapeFactory.get_task(\n",
    "            url, base_url=\"https://domo-support.domo.com\", debug_prn=False\n",
    "        ),\n",
    "    }\n",
    "    for url in test_urls\n",
    "]\n",
    "\n",
    "test_scrape_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US',\n",
       " 'base_url': 'https://domo-support.domo.com',\n",
       " 'download_folder': './SCRAPE',\n",
       " 'generate_filename_fn': <function __main__.generate_filename_from_url(url, base_folder=None, file_name=None) -> 'str'>,\n",
       " 'max_sleep_time': 10,\n",
       " 'scrape_config': Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f8ce44ee2a0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'),\n",
       " 'scrape_crawler': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task = test_scrape_tasks[2][\"config\"]\n",
    "test_task.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US to ./SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_generator = dg.DriverGenerator()\n",
    "driver = driver_generator.get_webdriver()\n",
    "\n",
    "test_task.scrape_page(driver=driver, is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Crawler\n",
    "\n",
    "The Scrape_Crawler is generally just a crawler that receieves a URL then manages scraping that page (by retrieving the correct scrape_configuration process) and searching for URLs to extract from that page and adding it to the `urls_to_visit` property.\n",
    "\n",
    "The ThreadPoolExecutor has a maximum number of threads, maximum_workers, it will use.\n",
    "\n",
    "This crawler does not concern itself with the return of the webcrawl task, because the task handles downloading the HTML file to a parameterized location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Crawler:\n",
    "    \"\"\"threadpool manager for crawling through a list of urls\"\"\"\n",
    "\n",
    "    executor: ThreadPoolExecutor\n",
    "    scrape_factory: Scrape_Factory\n",
    "    base_url: str\n",
    "\n",
    "    download_folder: str\n",
    "    visited_urls: set\n",
    "    urls_to_visit: set\n",
    "\n",
    "    driver_generator: dg.DriverGenerator = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        driver_path,\n",
    "        scrape_factory: Scrape_Factory,\n",
    "        urls_to_visit: list,\n",
    "        base_url: str,\n",
    "        urls_visited: list = None,\n",
    "        max_workers=5,\n",
    "        download_folder: str = \"./SCRAPE/\",\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.scrape_factory = scrape_factory\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.driver_generator = dg.DriverGenerator(driver_path=driver_path)\n",
    "        self.download_folder = download_folder\n",
    "\n",
    "        self.visited_urls = set()\n",
    "        if urls_visited:\n",
    "            [self._add_url_to_visited(url) for url in urls_visited]\n",
    "\n",
    "        self.urls_to_visit = set()\n",
    "\n",
    "        if urls_to_visit:\n",
    "            [self._add_url_to_visit(url) for url in urls_to_visit]\n",
    "\n",
    "    def _add_url_to_visit(self, url, debug_prn: bool = False):\n",
    "        \"\"\"adds a URL to the list of URLS to visit after testing that the URL has not already been visited\"\"\"\n",
    "\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            if debug_prn:\n",
    "                print(f\"adding {url} to to_vist list\")\n",
    "\n",
    "            self.urls_to_visit.add(url)\n",
    "            return self.urls_to_visit\n",
    "\n",
    "    def _add_url_to_visited(self, url, debug_prn: bool = False):\n",
    "        if url not in self.visited_urls:\n",
    "            if debug_prn:\n",
    "                print(f\"adding {url} to visited list\")\n",
    "\n",
    "            self.visited_urls.add(url)\n",
    "            return self.visited_urls\n",
    "\n",
    "    def _quit(self):\n",
    "        \"\"\"call when the executor queue is empty\"\"\"\n",
    "\n",
    "        self.executor.shutdown(wait=True)\n",
    "        return f\"Done scraping {len(self.visited_urls)} urls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_url': 'https://domo-support.domo.com',\n",
       " 'scrape_factory': Scrape_Factory(factory_configs=[Scrape_Config(pattern='.*/s/article/.*', content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f8ce44ec2c0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='class name', search_element_text='slds-form-element'), Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f8ce44ee2a0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'), Scrape_Config(pattern='.*/s/knowledge-base.*', content_extractor_fn=<function domokb_knowledgebase_content_extractor_fn at 0x7f8ce44eda80>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f8ce44ed620>, search_element_type='css selector', search_element_text='.topic-nav-container, .cDomoKBCategoryNav')]),\n",
       " 'executor': <concurrent.futures.thread.ThreadPoolExecutor at 0x7f8ce4287310>,\n",
       " 'driver_generator': <gdoc_sync.scraper.driver.DriverGenerator at 0x7f8ce4409490>,\n",
       " 'download_folder': './SCRAPE/',\n",
       " 'visited_urls': set(),\n",
       " 'urls_to_visit': {'https://domo-support.domo.com/s/article/36004740075?language=en_US',\n",
       "  'https://domo-support.domo.com/s/knowledge-base?language=en_US',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    urls_to_visit=test_urls,\n",
    ")\n",
    "wbs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Crawler)\n",
    "def crawl_urls(self: Scrape_Crawler, is_test: bool = False, debug_prn: bool = False):\n",
    "    while self.urls_to_visit:\n",
    "        url = self.urls_to_visit.pop()\n",
    "        self.visited_urls.add(url)\n",
    "\n",
    "        driver = self.driver_generator.get_webdriver()\n",
    "\n",
    "        task = self.scrape_factory.get_task(\n",
    "            url, debug_prn=debug_prn, scrape_crawler=self\n",
    "        )\n",
    "\n",
    "        future = self.executor.submit(task.scrape_page, driver=driver, is_test=is_test)\n",
    "        try:\n",
    "            future.result()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return self._quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/knowledge-base?language=en_US to ./SCRAPE/s_knowledge-base/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/knowledge-base to ./SCRAPE/s_knowledge-base/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes?language=en_US to ./SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK to ./SCRAPE/s_topic_0TO5w000000ZlOmGAK/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/36004740075?language=en_US to ./SCRAPE/s_article_36004740075/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023?language=en_US to ./SCRAPE/s_topic_0TO5w000000ZlOmGAK_20202023/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/7440921035671 to ./SCRAPE/s_article_7440921035671/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/4403173731863 to ./SCRAPE/s_article_4403173731863/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360043630093 to ./SCRAPE/s_article_360043630093/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000ZanLGAS to ./SCRAPE/s_topic_0TO5w000000ZanLGAS/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360042931894 to ./SCRAPE/s_article_360042931894/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360045163773 to ./SCRAPE/s_article_360045163773/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360042932554 to ./SCRAPE/s_article_360042932554/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360042930554 to ./SCRAPE/s_article_360042930554/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360042931614 to ./SCRAPE/s_article_360042931614/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360043436173 to ./SCRAPE/s_article_360043436173/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000ZapTGAS to ./SCRAPE/s_topic_0TO5w000000ZapTGAS/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360046750913 to ./SCRAPE/s_article_360046750913/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360043436113 to ./SCRAPE/s_article_360043436113/index.html\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360043437393 to ./SCRAPE/s_article_360043437393/index.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m wbs \u001b[38;5;241m=\u001b[39m Scrape_Crawler(\n\u001b[1;32m      2\u001b[0m     driver_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr//bin/chromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     scrape_factory\u001b[38;5;241m=\u001b[39mDomoKB_ScrapeFactory,\n\u001b[1;32m      4\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://domo-support.domo.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     urls_to_visit\u001b[38;5;241m=\u001b[39mtest_urls,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mwbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m wbs\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 20\u001b[0m, in \u001b[0;36mcrawl_urls\u001b[0;34m(self, is_test, debug_prn)\u001b[0m\n\u001b[1;32m     16\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m     17\u001b[0m     task\u001b[38;5;241m.\u001b[39mscrape_page, driver\u001b[38;5;241m=\u001b[39mdriver, is_test\u001b[38;5;241m=\u001b[39mis_test\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360043436053 to ./SCRAPE/s_article_360043436053/index.html\n"
     ]
    }
   ],
   "source": [
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    urls_to_visit=test_urls,\n",
    ")\n",
    "\n",
    "wbs.crawl_urls()\n",
    "\n",
    "wbs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
