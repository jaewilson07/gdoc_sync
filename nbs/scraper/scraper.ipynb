{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: scraper.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp scraper.Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Callable, Any\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from gdoc_sync.utils import upsert_folder, convert_str_file_name\n",
    "import gdoc_sync.scraper.driver as dg\n",
    "\n",
    "from nbdev.showdoc import patch_to, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from selenium.webdriver.common.by import By\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromeDriver 120.0.6099.109 (3419140ab665596f21b385ce136419fde0924272-refs/branch-heads/6099@{#1483})\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<html dir=\"ltr\" lang=\"en-US\"><head><title>Knowledge Base</title><meta content=\"default-src \\'self\\'; s'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from selenium.webdriver.common.by import By\n",
    "# import gdoc_sync.scraper.driver as dg\n",
    "\n",
    "test_urls = [\n",
    "    \"https://domo-support.domo.com/s/article/36004740075\",\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023\",  # list of articles\n",
    "    \"https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes\",  # list of topics\n",
    "    \"https://domo-support.domo.com/s/knowledge-base\",\n",
    "]\n",
    "\n",
    "drivergenerator = dg.DriverGenerator(debug_prn=True)\n",
    "driver = drivergenerator.get_webdriver()\n",
    "\n",
    "test_soup = dg.get_pagesource(\n",
    "    driver=driver,\n",
    "    url=test_urls[3],\n",
    "    search_criteria_tuple=(By.CLASS_NAME, \"topic-nav-container\"),\n",
    "    max_sleep_time=15,\n",
    "    return_soup=True,\n",
    ")\n",
    "\n",
    "# test_soup = dg.get_pagesource(\n",
    "#     driver=driver,\n",
    "#     url=test_urls[2],\n",
    "#     search_criteria_tuple=(\n",
    "#         By.CSS_SELECTOR,\n",
    "#         f\".{', .'.join(['section-list-item', 'article-list-item'] )}\",\n",
    "#     ),\n",
    "#     max_sleep_time=15,\n",
    "#     return_soup=True,\n",
    "# )\n",
    "\n",
    "str(test_soup)[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils for Processing Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def remove_query_params_from_url(url):\n",
    "    u = urlparse(url)\n",
    "    return urljoin(url, urlparse(url).path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/article/36004740075',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes',\n",
       " 'https://domo-support.domo.com/s/knowledge-base']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[remove_query_params_from_url(url) for url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_links(\n",
    "    soup: BeautifulSoup,\n",
    "    base_url: str = None,\n",
    "    custom_link_extractor_fn: Callable = None,  # can add custom function for handling URLs\n",
    "    debug_prn: bool = False,\n",
    ") -> [str]:\n",
    "    \"\"\"returns a list of urls\"\"\"\n",
    "\n",
    "    links_ls = []\n",
    "\n",
    "    for link in soup.findAll(\"a\"):\n",
    "        if not link.has_attr(\"href\"):\n",
    "            continue\n",
    "\n",
    "        url = link[\"href\"]\n",
    "\n",
    "        if debug_prn:\n",
    "            print(url)\n",
    "\n",
    "        if url.startswith(\"/\") and base_url:\n",
    "            url = urljoin(base_url, url)\n",
    "\n",
    "        if base_url and not url.startswith(base_url):\n",
    "            continue\n",
    "\n",
    "        if custom_link_extractor_fn:\n",
    "            url = custom_link_extractor_fn(url, base_url)\n",
    "\n",
    "        if url in links_ls or not url:\n",
    "            continue\n",
    "\n",
    "        links_ls.append(url)\n",
    "\n",
    "    return list(set(links_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/knowledge-base?language=en_US',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC/release-notes?language=en_US']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_links(\n",
    "    test_soup,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    debug_prn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def generate_filename_from_url(url, download_folder=None, file_name=None) -> str:\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    file_path = \"_\".join([str for str in parsed_url[2].split(\"/\") if str])\n",
    "\n",
    "    if download_folder:\n",
    "        file_path = os.path.join(download_folder, file_path)\n",
    "\n",
    "    if file_name:\n",
    "        file_path = os.path.join(file_path, file_name)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s_article_36004740075',\n",
      " 's_topic_0TO5w000000ZlOmGAK_20202023',\n",
      " 's_topic_0TO5w000000Zan7GAC_archived-feature-release-notes',\n",
      " 's_knowledge-base']\n",
      "\n",
      "-- alternative with base_url and different file name\n",
      "\n",
      "['/SCRAPE/s_article_36004740075/index.html',\n",
      " '/SCRAPE/s_topic_0TO5w000000ZlOmGAK_20202023/index.html',\n",
      " '/SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html',\n",
      " '/SCRAPE/s_knowledge-base/index.html']\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "pprint([generate_filename_from_url(url) for url in test_urls])\n",
    "\n",
    "print(\"\")\n",
    "print(\"-- alternative with base_url and different file name\")\n",
    "print(\"\")\n",
    "\n",
    "pprint([generate_filename_from_url(url, \"/SCRAPE\", \"index.html\") for url in test_urls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Config:\n",
    "    \"\"\"class for collating data about how to scrape a page\n",
    "    pass a list of scrape_config into ScrapeFactory\n",
    "    \"\"\"\n",
    "\n",
    "    pattern: re.Pattern  # pattern for matching URLs to appropriate config\n",
    "    content_extractor_fn: Callable = None  # function for the subset of HTML to extract\n",
    "    link_extractor_fn: Callable = (\n",
    "        extract_links  # function for extracting links from soup\n",
    "    )\n",
    "\n",
    "    search_element_type: Any = None  # from selenium.webdriver.common.by import By\n",
    "    search_element_text: str = None\n",
    "\n",
    "    def get_search_tuple(self) -> Any:\n",
    "        \"\"\"used by dg.get_pagesource() function to wait if rendered page has rendered\"\"\"\n",
    "\n",
    "        if not self.search_element_text and self.search_element_type:\n",
    "            return None\n",
    "\n",
    "        return (self.search_element_type, self.search_element_text)\n",
    "\n",
    "    def is_text_match_pattern(self, text, debug_prn: bool = False):\n",
    "        pattern = re.compile(self.pattern)\n",
    "\n",
    "        match_pattern = pattern.match(text)\n",
    "\n",
    "        if debug_prn:\n",
    "            print({\"text\": text, \"pattern\": self.pattern})\n",
    "\n",
    "        if not match_pattern:\n",
    "            return False\n",
    "\n",
    "        if debug_prn:\n",
    "            print(match_pattern)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def __id__(self, other):\n",
    "        return self.pattern == other.pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/gdoc_sync/blob/main/gdoc_sync/scraper/Scraper.py#L84){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Scrape_Config\n",
       "\n",
       ">      Scrape_Config (pattern:re.Pattern, content_extractor_fn:Callable=None,\n",
       ">                     link_extractor_fn:Callable=<function extract_links>,\n",
       ">                     search_element_type:Any=None,\n",
       ">                     search_element_text:str=None)\n",
       "\n",
       "class for collating data about how to scrape a page\n",
       "pass a list of scrape_config into ScrapeFactory"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/gdoc_sync/blob/main/gdoc_sync/scraper/Scraper.py#L84){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Scrape_Config\n",
       "\n",
       ">      Scrape_Config (pattern:re.Pattern, content_extractor_fn:Callable=None,\n",
       ">                     link_extractor_fn:Callable=<function extract_links>,\n",
       ">                     search_element_type:Any=None,\n",
       ">                     search_element_text:str=None)\n",
       "\n",
       "class for collating data about how to scrape a page\n",
       "pass a list of scrape_config into ScrapeFactory"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Scrape_Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ScrapeTask_NoDriverProvided(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Driver not provided\")\n",
    "\n",
    "\n",
    "class ScrapeTask_NoBaseUrl(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            \"no base_url provided to scrape_task. must run as is_test = True\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ScrapeTask_NoScrapeCrawler(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            \"no scrape_crawler provided.  must run scrape_page as is_test = True\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Task:\n",
    "    \"\"\"scrape_factory matches a URL to a scrape_config and returns a scrape_task\"\"\"\n",
    "\n",
    "    url: str  # will be assigned after retrieved from scrape_factory\n",
    "\n",
    "    base_url: str = None\n",
    "    download_folder: str = None\n",
    "\n",
    "    generate_filename_fn: Callable = generate_filename_from_url\n",
    "\n",
    "    max_sleep_time: int = 10\n",
    "    scrape_config: Scrape_Config = None\n",
    "    scrape_crawler: Scrape_Crawler = None  # will be optional parent threadpool manager.\n",
    "\n",
    "    @classmethod\n",
    "    def _from_factory(\n",
    "        cls,\n",
    "        url,\n",
    "        base_url,\n",
    "        scrape_config,\n",
    "        scrape_crawler=None,\n",
    "        download_folder=None,\n",
    "    ):\n",
    "        return cls(\n",
    "            url=url,\n",
    "            base_url=base_url,\n",
    "            download_folder=download_folder,\n",
    "            scrape_config=scrape_config,\n",
    "            scrape_crawler=scrape_crawler,\n",
    "        )\n",
    "\n",
    "    def _get_pagesource(self, driver=None, debug_prn: bool = False):\n",
    "        \"\"\"\n",
    "        handles fetching the pagesource, html content of a URL\n",
    "        \"\"\"\n",
    "\n",
    "        url = self.url\n",
    "\n",
    "        driver = (\n",
    "            driver\n",
    "            or (\n",
    "                self.scrape_crawler\n",
    "                and self.scrape_crawler.driver_generator.get_driver()\n",
    "            )\n",
    "            or None\n",
    "        )\n",
    "\n",
    "        if not driver:\n",
    "            raise ScrapeTask_NoDriverProvided()\n",
    "\n",
    "        search_criteria_tuple = (\n",
    "            self.scrape_config and self.scrape_config.get_search_tuple()\n",
    "        ) or None\n",
    "\n",
    "        max_sleep_time = self.max_sleep_time or 15\n",
    "\n",
    "        if debug_prn:\n",
    "            print(\n",
    "                {\n",
    "                    \"url\": url,\n",
    "                    \"max_sleep_time\": max_sleep_time,\n",
    "                    \"search_criteria\": search_criteria_tuple,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return dg.get_pagesource(\n",
    "            url=url,\n",
    "            search_criteria_tuple=search_criteria_tuple,\n",
    "            driver=driver,\n",
    "            max_sleep_time=max_sleep_time,\n",
    "        )\n",
    "\n",
    "    def _download_content(self, file_name, content):\n",
    "        dir_name = os.path.dirname(file_name)\n",
    "\n",
    "        if dir_name[-1] != \"/\":\n",
    "            dir_name += \"/\"\n",
    "\n",
    "        upsert_folder(dir_name)\n",
    "\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(content))\n",
    "\n",
    "        return file_name\n",
    "\n",
    "    def _update_crawler(self, soup, base_url, scrape_crawler):\n",
    "        \"\"\"if config is part of a scrape_crawler / managed threadpool will update crawler's urls_to_visit list.\"\"\"\n",
    "\n",
    "        scrape_crawler._add_url_to_visited(self.url)\n",
    "\n",
    "        urls_to_visit = self.scrape_config.link_extractor_fn(soup, base_url)\n",
    "\n",
    "        [scrape_crawler._add_url_to_visit(url) for url in urls_to_visit]\n",
    "\n",
    "        return urls_to_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Task)\n",
    "def execute(\n",
    "    self: Scrape_Task,\n",
    "    base_url=None,\n",
    "    driver=None,\n",
    "    download_folder=None,\n",
    "    debug_prn: bool = False,\n",
    "    is_suppress_errors: bool = False,\n",
    "    is_test: bool = False,\n",
    "):\n",
    "    \"\"\"handles executing the scrape_task\n",
    "    1. get pagesource\n",
    "    2. download index\n",
    "    3. download content\n",
    "    4. update crawler\n",
    "    - get_links from pagesource\n",
    "    \"\"\"\n",
    "    url = self.url\n",
    "\n",
    "    download_folder = download_folder or self.download_folder or \"./SCRAPE\"\n",
    "\n",
    "    scrape_crawler = self.scrape_crawler\n",
    "\n",
    "    driver = (\n",
    "        driver\n",
    "        or (scrape_crawler and scrape_crawler.driver_generator.get_webdriver())\n",
    "        or None\n",
    "    )\n",
    "\n",
    "    if not driver:\n",
    "        raise ScrapeTask_NoDriverProvided()\n",
    "\n",
    "    base_url = (\n",
    "        base_url\n",
    "        or self.base_url\n",
    "        or (scrape_crawler and scrape_crawler.base_url)\n",
    "        or None\n",
    "    )\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"scraping_page {url}\")\n",
    "\n",
    "    try:\n",
    "        soup = self._get_pagesource(driver=driver, debug_prn=debug_prn)\n",
    "\n",
    "        # download index\n",
    "        file_name = self.generate_filename_fn(\n",
    "            url=url, download_folder=download_folder, file_name=\"index.html\"\n",
    "        )\n",
    "\n",
    "        save_location = self._download_content(file_name=file_name, content=soup)\n",
    "\n",
    "        # download content\n",
    "        if self.scrape_config:\n",
    "            content = self.scrape_config.content_extractor_fn(soup)\n",
    "\n",
    "            content_name = self.generate_filename_fn(\n",
    "                url=url, download_folder=download_folder, file_name=\"content.html\"\n",
    "            )\n",
    "\n",
    "            self._download_content(\n",
    "                content_name,\n",
    "                content=content,\n",
    "            )\n",
    "\n",
    "        print(f\"🎉 successfully scraped {url} to {save_location}\")\n",
    "\n",
    "        if is_test:\n",
    "            return []\n",
    "\n",
    "        # update crawler\n",
    "        if not base_url:\n",
    "            raise ScrapeTask_NoBaseUrl()\n",
    "\n",
    "        if not scrape_crawler:\n",
    "            raise ScrapeTask_NoScrapeCrawler()\n",
    "\n",
    "        \"\"\"if config is part of a scrape_crawler / managed threadpool will update crawler's urls_to_visit list.\"\"\"\n",
    "        return self._update_crawler(\n",
    "            soup=soup, base_url=base_url, scrape_crawler=scrape_crawler\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        if not is_suppress_errors:\n",
    "            raise (e)\n",
    "        return f\"💀 failed to download {url} received errror{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html dir=\"ltr\" lang=\"en-US\"><head><title>Article Detail</title><meta content=\"default-src \\'self\\'; s'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Scrape_Task(\n",
    "    url=test_urls[0],\n",
    ")\n",
    "\n",
    "str(task._get_pagesource(driver=driver))[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes to ./SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Scrape_Task(\n",
    "    url=test_urls[2],\n",
    ")\n",
    "\n",
    "task.execute(driver=driver, base_url=\"https://domo-support.domo.com\", is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Factory converts URL into Scrape_Config task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Scrape_Factory_NoConfigMatch(Exception):\n",
    "    def __init__(self, text):\n",
    "        super().__init__(\n",
    "            f\"{text} has no pattern match in factory_configs, add an appropriate config or check pattern matches\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Factory:\n",
    "    \"\"\"class handles a list of Scrape_Configs and will return the 'correct one' given a URL\"\"\"\n",
    "\n",
    "    factory_configs: List[Scrape_Config]\n",
    "\n",
    "    def get_task(\n",
    "        self,\n",
    "        url,\n",
    "        download_folder=\"./SCRAPE\",\n",
    "        base_url=None,\n",
    "        scrape_crawler=None,\n",
    "        debug_prn: bool = False,\n",
    "    ):\n",
    "        config = next(\n",
    "            (\n",
    "                config\n",
    "                for config in self.factory_configs\n",
    "                if config.is_text_match_pattern(url, debug_prn=debug_prn)\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if not config:\n",
    "            raise Scrape_Factory_NoConfigMatch(text=url)\n",
    "\n",
    "        return Scrape_Task._from_factory(\n",
    "            scrape_config=config,\n",
    "            url=url,\n",
    "            base_url=base_url,\n",
    "            download_folder=download_folder,\n",
    "            scrape_crawler=scrape_crawler,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeFactory\n",
    "\n",
    "An instance of Scrape Factory for scraping Domo KBs three types of pages, Articles, Topic lists, and the main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def process_domo_kb_link(url, base_url):\n",
    "    # remove query params\n",
    "    url = remove_query_params_from_url(url)\n",
    "\n",
    "    if not url or not \"/s/\" in url:\n",
    "        return None\n",
    "\n",
    "    # only keep the first 6 pieces of the URL\n",
    "    url = \"/\".join(url.split(\"/\")[:6])\n",
    "\n",
    "    if url.endswith(\"/\"):\n",
    "        url = url[:-1]\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def domokb_link_extractor_fn(soup, base_url, debug_prn: bool = False):\n",
    "    \"\"\"custom link extractor for processing Domo KBs.\n",
    "    will be embedded into all DomoKB_ScrapeConfigs\n",
    "    \"\"\"\n",
    "    return extract_links(\n",
    "        soup,\n",
    "        custom_link_extractor_fn=process_domo_kb_link,\n",
    "        base_url=base_url,\n",
    "        debug_prn=debug_prn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://domo-support.domo.com/s/knowledge-base',\n",
       " 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "domokb_link_extractor_fn(test_soup, \"https://domo-support.domo.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def domokb_article_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"article-column\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Article = Scrape_Config(\n",
    "    pattern=r\".*/s/article/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_article_content_extractor_fn,\n",
    "    search_element_type=By.CLASS_NAME,\n",
    "    search_element_text=\"slds-form-element\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def domokb_topic_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"knowledge-base\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_Topic = Scrape_Config(\n",
    "    pattern=r\".*/s/topic/.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_topic_content_extractor_fn,\n",
    "    search_element_type=By.CSS_SELECTOR,\n",
    "    search_element_text=f\".{', .'.join(['section-list-item', 'article-list-item'] )}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeConfig_NavContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def domokb_knowledgebase_content_extractor_fn(soup) -> BeautifulSoup:\n",
    "    return soup.find(class_=[\"knowledge-base\"])\n",
    "\n",
    "\n",
    "DomoKB_ScrapeConfig_KnowledgeBase = Scrape_Config(\n",
    "    pattern=r\".*/s/knowledge-base.*\",\n",
    "    link_extractor_fn=domokb_link_extractor_fn,\n",
    "    content_extractor_fn=domokb_knowledgebase_content_extractor_fn,\n",
    "    search_element_type=By.CSS_SELECTOR,\n",
    "    search_element_text=f\".{', .'.join(['topic-nav-container', 'cDomoKBCategoryNav'] )}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DomoKB_ScrapeFactory\n",
    "A factory pattern receives a parameter (the url to match to) then returns the appropriate implementation `Scrape_Config` for handling that url.\n",
    "\n",
    "This pattern could be extended for any type of website; however we have implemented a factory specifically for handling Domo Kbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "DomoKB_ScrapeFactory = Scrape_Factory(\n",
    "    [\n",
    "        DomoKB_ScrapeConfig_Article,\n",
    "        DomoKB_ScrapeConfig_Topic,\n",
    "        DomoKB_ScrapeConfig_KnowledgeBase,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation\n",
    "\n",
    "Of retrieving the scrape_config using the `scrape_factory.get_factory_config() method\n",
    "\n",
    "- test_urls is a list of URLS\n",
    "- use a list comprenension to show we can retrieve the correct config for different URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://domo-support.domo.com/s/article/36004740075',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/article/36004740075', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f96df933e20>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/article/.*', content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f96df7ba160>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='class name', search_element_text='slds-form-element'), scrape_crawler=None)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f96df933e20>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f96df7ba5c0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'), scrape_crawler=None)},\n",
       " {'url': 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f96df933e20>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f96df7ba5c0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'), scrape_crawler=None)},\n",
       " {'url': 'https://domo-support.domo.com/s/knowledge-base',\n",
       "  'config': Scrape_Task(url='https://domo-support.domo.com/s/knowledge-base', base_url='https://domo-support.domo.com', download_folder='./SCRAPE', generate_filename_fn=<function generate_filename_from_url at 0x7f96df933e20>, max_sleep_time=10, scrape_config=Scrape_Config(pattern='.*/s/knowledge-base.*', content_extractor_fn=<function domokb_knowledgebase_content_extractor_fn at 0x7f96df7ba8e0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='css selector', search_element_text='.topic-nav-container, .cDomoKBCategoryNav'), scrape_crawler=None)}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scrape_tasks = [\n",
    "    {\n",
    "        \"url\": url,\n",
    "        \"config\": DomoKB_ScrapeFactory.get_task(\n",
    "            url, base_url=\"https://domo-support.domo.com\", debug_prn=False\n",
    "        ),\n",
    "    }\n",
    "    for url in test_urls\n",
    "]\n",
    "\n",
    "test_scrape_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes',\n",
       " 'base_url': 'https://domo-support.domo.com',\n",
       " 'download_folder': './SCRAPE',\n",
       " 'generate_filename_fn': <function __main__.generate_filename_from_url(url, download_folder=None, file_name=None) -> 'str'>,\n",
       " 'max_sleep_time': 10,\n",
       " 'scrape_config': Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f96df7ba5c0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'),\n",
       " 'scrape_crawler': None}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task = test_scrape_tasks[2][\"config\"]\n",
    "test_task.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes to ./SCRAPE/s_topic_0TO5w000000Zan7GAC_archived-feature-release-notes/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_generator = dg.DriverGenerator()\n",
    "driver = driver_generator.get_webdriver()\n",
    "\n",
    "test_task.execute(driver=driver, is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape_Crawler\n",
    "\n",
    "The Scrape_Crawler is generally just a crawler that receieves a URL then manages scraping that page (by retrieving the correct scrape_configuration process) and searching for URLs to extract from that page and adding it to the `urls_to_visit` property.\n",
    "\n",
    "The ThreadPoolExecutor has a maximum number of threads, maximum_workers, it will use.\n",
    "\n",
    "This crawler does not concern itself with the return of the webcrawl task, because the task handles downloading the HTML file to a parameterized location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scrape_Crawler:\n",
    "    \"\"\"threadpool manager for crawling through a list of urls\"\"\"\n",
    "\n",
    "    executor: ThreadPoolExecutor\n",
    "    scrape_factory: Scrape_Factory\n",
    "    base_url: str\n",
    "\n",
    "    download_folder: str\n",
    "    visited_urls: set\n",
    "    urls_to_visit: set\n",
    "\n",
    "    driver_generator: dg.DriverGenerator = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        driver_path,\n",
    "        scrape_factory: Scrape_Factory,\n",
    "        urls_to_visit: list,\n",
    "        base_url: str,\n",
    "        urls_visited: list = None,\n",
    "        max_workers=5,\n",
    "        download_folder: str = \"./SCRAPE/\",\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.scrape_factory = scrape_factory\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.driver_generator = dg.DriverGenerator(driver_path=driver_path)\n",
    "        self.download_folder = download_folder\n",
    "\n",
    "        self.visited_urls = set()\n",
    "        if urls_visited:\n",
    "            [self._add_url_to_visited(url) for url in urls_visited]\n",
    "\n",
    "        self.urls_to_visit = set()\n",
    "\n",
    "        if urls_to_visit:\n",
    "            [self._add_url_to_visit(url) for url in urls_to_visit]\n",
    "\n",
    "    def _add_url_to_visit(self, url, debug_prn: bool = False):\n",
    "        \"\"\"adds a URL to the list of URLS to visit after testing that the URL has not already been visited\"\"\"\n",
    "\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            if debug_prn:\n",
    "                print(f\"adding {url} to to_vist list\")\n",
    "\n",
    "            self.urls_to_visit.add(url)\n",
    "            return self.urls_to_visit\n",
    "\n",
    "    def _add_url_to_visited(self, url, debug_prn: bool = False):\n",
    "        if url not in self.visited_urls:\n",
    "            if debug_prn:\n",
    "                print(f\"adding {url} to visited list\")\n",
    "\n",
    "            self.visited_urls.add(url)\n",
    "            return self.visited_urls\n",
    "\n",
    "    def _quit(self):\n",
    "        \"\"\"call when the executor queue is empty\"\"\"\n",
    "\n",
    "        self.executor.shutdown(wait=True)\n",
    "        return f\"Done scraping {len(self.visited_urls)} urls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_url': 'https://domo-support.domo.com',\n",
       " 'scrape_factory': Scrape_Factory(factory_configs=[Scrape_Config(pattern='.*/s/article/.*', content_extractor_fn=<function domokb_article_content_extractor_fn at 0x7f96df7ba160>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='class name', search_element_text='slds-form-element'), Scrape_Config(pattern='.*/s/topic/.*', content_extractor_fn=<function domokb_topic_content_extractor_fn at 0x7f96df7ba5c0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='css selector', search_element_text='.section-list-item, .article-list-item'), Scrape_Config(pattern='.*/s/knowledge-base.*', content_extractor_fn=<function domokb_knowledgebase_content_extractor_fn at 0x7f96df7ba8e0>, link_extractor_fn=<function domokb_link_extractor_fn at 0x7f96df9320c0>, search_element_type='css selector', search_element_text='.topic-nav-container, .cDomoKBCategoryNav')]),\n",
       " 'executor': <concurrent.futures.thread.ThreadPoolExecutor at 0x7f96df52c050>,\n",
       " 'driver_generator': <gdoc_sync.scraper.driver.DriverGenerator at 0x7f96df52f350>,\n",
       " 'download_folder': './SCRAPE/',\n",
       " 'visited_urls': set(),\n",
       " 'urls_to_visit': {'https://domo-support.domo.com/s/article/36004740075',\n",
       "  'https://domo-support.domo.com/s/knowledge-base',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC/archived-feature-release-notes',\n",
       "  'https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023'}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    urls_to_visit=test_urls,\n",
    ")\n",
    "wbs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch_to(Scrape_Crawler)\n",
    "def crawl_urls(self: Scrape_Crawler, is_test: bool = False, debug_prn: bool = False):\n",
    "    while self.urls_to_visit:\n",
    "        url = self.urls_to_visit.pop()\n",
    "        self.visited_urls.add(url)\n",
    "\n",
    "        driver = self.driver_generator.get_webdriver()\n",
    "\n",
    "        task = self.scrape_factory.get_task(\n",
    "            url, debug_prn=debug_prn, scrape_crawler=self\n",
    "        )\n",
    "\n",
    "        future = self.executor.submit(task.execute, driver=driver, is_test=is_test)\n",
    "        try:\n",
    "            future.result()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return self._quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/knowledge-base to ./SCRAPE/s_knowledge-base/index.html\n",
      "['https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC']\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/36004740075 to ./SCRAPE/s_article_36004740075/index.html\n",
      "['https://domo-support.domo.com/s/article/36004740075', 'https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/article/7440921035671', 'https://domo-support.domo.com/s/article/360043630093', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC', 'https://domo-support.domo.com/s/article/360043429693', 'https://domo-support.domo.com/s/article/360043429933', 'https://domo-support.domo.com/s/article/000005166', 'https://domo-support.domo.com/s/topic/0TO5w000000ZmOBGA0', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/article/360043931814', 'https://domo-support.domo.com/s/article/7872485267991', 'https://domo-support.domo.com/s/article/360043429953']\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/7440921035671 to ./SCRAPE/s_article_7440921035671/index.html\n",
      "['https://domo-support.domo.com/s/article/36004740075', 'https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/article/7440921035671', 'https://domo-support.domo.com/s/article/360043630093', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC', 'https://domo-support.domo.com/s/article/360043429933', 'https://domo-support.domo.com/s/topic/0TO5w000000ZmOBGA0', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/article/360043931814', 'https://domo-support.domo.com/s/article/360043429693', 'https://domo-support.domo.com/s/article/360043429953']\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000ZlOmGAK/20202023 to ./SCRAPE/s_topic_0TO5w000000ZlOmGAK_20202023/index.html\n",
      "['https://domo-support.domo.com/s/article/360061873754', 'https://domo-support.domo.com/s/article/000005256', 'https://domo-support.domo.com/s/article/4409045382935', 'https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/article/4425111066903', 'https://domo-support.domo.com/s/article/360055244114', 'https://domo-support.domo.com/s/article/7579340458903', 'https://domo-support.domo.com/s/article/4403173731863', 'https://domo-support.domo.com/s/article/360050912013', 'https://domo-support.domo.com/s/article/000005174', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC', 'https://domo-support.domo.com/s/article/9645646505111', 'https://domo-support.domo.com/s/article/000005377']\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360043630093 to ./SCRAPE/s_article_360043630093/index.html\n",
      "['https://domo-support.domo.com/s/article/360043434973', 'https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/article/360043630093', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/article/360043436013', 'https://domo-support.domo.com/s/article/360042931774', 'https://domo-support.domo.com/s/article/360042929094', 'https://domo-support.domo.com/s/article/36004740075', 'https://domo-support.domo.com/s/article/360042932034', 'https://domo-support.domo.com/s/article/360042931874', 'https://domo-support.domo.com/s/article/360043436233', 'https://domo-support.domo.com/s/article/360042931614', 'https://domo-support.domo.com/s/article/360042931894', 'https://domo-support.domo.com/s/article/360043436113', 'https://domo-support.domo.com/s/article/360042932074', 'https://domo-support.domo.com/s/article/360043931814', 'https://domo-support.domo.com/s/article/360043433413', 'https://domo-support.domo.com/s/article/360042929114', 'https://domo-support.domo.com/s/topic/0TO5w000000ZammGAC', 'https://domo-support.domo.com/s/article/360042931694', 'https://domo-support.domo.com/s/article/360043436173', 'https://domo-support.domo.com/s/article/360043436273', 'https://domo-support.domo.com/s/article/360043433393', 'https://domo-support.domo.com/s/article/360042930554', 'https://domo-support.domo.com/s/article/360042930534', 'https://domo-support.domo.com/s/article/360043436513', 'https://domo-support.domo.com/s/article/360043436293', 'https://domo-support.domo.com/s/article/360043429953', 'https://domo-support.domo.com/s/article/360042931754', 'https://domo-support.domo.com/s/topic/0TO5w000000ZanLGAS', 'https://domo-support.domo.com/s/article/360042932974', 'https://domo-support.domo.com/s/article/360043429933', 'https://domo-support.domo.com/s/article/360043436053', 'https://domo-support.domo.com/s/article/360043436213', 'https://domo-support.domo.com/s/article/360043439173', 'https://domo-support.domo.com/s/article/360043433113', 'https://domo-support.domo.com/s/article/360043436193', 'https://domo-support.domo.com/s/article/360043436033', 'https://domo-support.domo.com/s/article/360042931734', 'https://domo-support.domo.com/s/article/360043436253', 'https://domo-support.domo.com/s/topic/0TO5w000000ZaoyGAC', 'https://domo-support.domo.com/s/article/7963308557463']\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC to ./SCRAPE/s_topic_0TO5w000000ZamzGAC/index.html\n",
      "['https://domo-support.domo.com/s/topic/0TO5w000000ZanUGAS', 'https://domo-support.domo.com/s/topic/0TO5w000000ZanvGAC', 'https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/topic/0TO5w000000ZanVGAS', 'https://domo-support.domo.com/s/topic/0TO5w000000ZaniGAC', 'https://domo-support.domo.com/s/topic/0TO5w000000ZmOBGA0', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/topic/0TO5w000000Zao9GAC', 'https://domo-support.domo.com/s/topic/0TO5w000000ZanWGAS']\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC to ./SCRAPE/s_topic_0TO5w000000ZamwGAC/index.html\n",
      "['https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/article/Current-Release-Notes', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/topic/0TO5w000000Zan7GAC']\n",
      "ERROR: Message: \n",
      "Stacktrace:\n",
      "#0 0x55a935543f83 <unknown>\n",
      "#1 0x55a9351fccf7 <unknown>\n",
      "#2 0x55a93524c99e <unknown>\n",
      "#3 0x55a93524caa1 <unknown>\n",
      "#4 0x55a935297d64 <unknown>\n",
      "#5 0x55a9352760dd <unknown>\n",
      "#6 0x55a935295006 <unknown>\n",
      "#7 0x55a935275e53 <unknown>\n",
      "#8 0x55a93523ddd4 <unknown>\n",
      "#9 0x55a93523f1de <unknown>\n",
      "#10 0x55a935508531 <unknown>\n",
      "#11 0x55a93550c455 <unknown>\n",
      "#12 0x55a9354f4f55 <unknown>\n",
      "#13 0x55a93550d0ef <unknown>\n",
      "#14 0x55a9354d899f <unknown>\n",
      "#15 0x55a935531008 <unknown>\n",
      "#16 0x55a9355311d7 <unknown>\n",
      "#17 0x55a935543124 <unknown>\n",
      "#18 0x7f2710b23ac3 <unknown>\n",
      " -  https://domo-support.domo.com/s/article/360043436013 failed to load page within 10 seconds.  is the element represented in the element list?\n",
      "'NoneType' object has no attribute 'find'\n",
      "🎉 successfully scraped https://domo-support.domo.com/s/topic/0TO5w000000Zao9GAC to ./SCRAPE/s_topic_0TO5w000000Zao9GAC/index.html\n",
      "['https://domo-support.domo.com/s/article/360063698733', 'https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/article/360043428133', 'https://domo-support.domo.com/s/article/360042922974', 'https://domo-support.domo.com/s/article/360042922994', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamzGAC', 'https://domo-support.domo.com/s/article/4410213098903', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/article/360042922954', 'https://domo-support.domo.com/s/article/360043427793', 'https://domo-support.domo.com/s/article/360042923014']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m wbs \u001b[38;5;241m=\u001b[39m Scrape_Crawler(\n\u001b[1;32m      2\u001b[0m     driver_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr//bin/chromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     scrape_factory\u001b[38;5;241m=\u001b[39mDomoKB_ScrapeFactory,\n\u001b[1;32m      4\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://domo-support.domo.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     urls_to_visit\u001b[38;5;241m=\u001b[39mtest_urls,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mwbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m wbs\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "Cell \u001b[0;32mIn[123], line 18\u001b[0m, in \u001b[0;36mcrawl_urls\u001b[0;34m(self, is_test, debug_prn)\u001b[0m\n\u001b[1;32m     16\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39msubmit(task\u001b[38;5;241m.\u001b[39mexecute, driver\u001b[38;5;241m=\u001b[39mdriver, is_test\u001b[38;5;241m=\u001b[39mis_test)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 successfully scraped https://domo-support.domo.com/s/article/360042931774 to ./SCRAPE/s_article_360042931774/index.html\n",
      "['https://domo-support.domo.com/s/article/36004740075', 'https://domo-support.domo.com/s/knowledge-base', 'https://domo-support.domo.com/s/topic/0TO5w000000ZammGAC', 'https://domo-support.domo.com/s/article/360042926054', 'https://domo-support.domo.com/s/article/360043630093', 'https://domo-support.domo.com/s/topic/0TO5w000000ZanLGAS', 'https://domo-support.domo.com/s/topic/0TO5w000000ZaojGAC', 'https://domo-support.domo.com/s/article/360042926274', 'https://domo-support.domo.com/s/article/360043429933', 'https://domo-support.domo.com/s/topic/0TO5w000000ZamwGAC', 'https://domo-support.domo.com/s/article/360043931814', 'https://domo-support.domo.com/s/article/360042931774', 'https://domo-support.domo.com/s/article/360043429953']\n"
     ]
    }
   ],
   "source": [
    "wbs = Scrape_Crawler(\n",
    "    driver_path=\"/usr//bin/chromedriver\",\n",
    "    scrape_factory=DomoKB_ScrapeFactory,\n",
    "    base_url=\"https://domo-support.domo.com\",\n",
    "    urls_to_visit=test_urls,\n",
    ")\n",
    "\n",
    "wbs.crawl_urls()\n",
    "\n",
    "wbs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
